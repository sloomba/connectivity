{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wish for a well-connected society? Have no more than $2\\sqrt{|\\mathrm{friends}|-1}$ number of friends who are similar to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stochastic Block Model\n",
    "\n",
    "Previously, we saw how in the case of a 1-dimensional categorical Blau space, the logistic connectivity kernel can be better represented as a stochastic block model (SBM), wherein nodes are members of a certain community $i \\in \\{1,2\\dots k\\}$ in the given Blau dimension, whose members form friendships according to a symmetric community-level probability matrix $\\Psi=\\{\\psi_{ij}\\}_{i,j=1}^n$.\n",
    "\n",
    "Let us consider a 1-dimensional categorical Blau space with $n$ people in $k$ communities. For each person $x_i \\in \\mathcal{X}$, we can write a corresponding membership vector $z_i \\in \\{0,1\\}^k$ containing exactly one $1$. (We could potentially relax this to mixed-membership vectors $z_i \\in [0,1]^k$ that sum to $1$.) Correspondingly, consider the assignment matrix $Z \\in \\{0,1\\}^{n\\times k}$. Then the stoachastic block model essentially suggests.\n",
    "\n",
    "$$A \\sim\\mathrm{Bernoulli}(Z\\Psi Z^T)$$\n",
    "\n",
    "Note that this can lead to self-loops, which we would ideally not like to have in the graph. Let us write instead for the \"simple\" graph $\\hat{A}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbb{E}[A|Z,\\Psi] &= Z\\Psi Z^T \\\\\n",
    "\\mathbb{E}[\\hat{A}|A] &= \\mathbb{E}[A|Z,\\Psi] - \\mathrm{diag}(\\mathbb{E}[A|Z,\\Psi])\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "1. Clearly, the $\\mathrm{rank}(A) \\le \\mathrm{min}(\\mathrm{rank}(Z), \\mathrm{rank}(\\Psi))$, which implies $\\mathrm{rank}(A) \\le k$. This satisfies our intuition that the adjacency matrix of an SBM model probably has a basis of the order of number of communities, and not number of people. This means that we can effectively reduce problems concerning with the adjacency matrix down to the level of the probability matrix instead, as we attempt to do below.\n",
    "2. Since this is a probabilistic model, we will mostly talk of the *expectation* of the adjacency matrix below.\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "Say we have somehow estimated the SBM matrix $\\Psi$, and are interested in the eigenspectrum of the corresponding (expected) adjacency matrix (more importantly perhaps the Laplacian matrix since it captures diffusion on the network). The above note suggests we should be able to do this at the level of $\\Psi$. For notational convenience, we'll indicate $\\mathbb{E}[A|Z,\\Psi]$ simply as $A = Z\\Psi Z^T$. (In essence, this treats the network as a weighted one.)\n",
    "\n",
    "First, note that we can decompose $Z$ using singular value decomposition (SVD) as:\n",
    "\n",
    "\\begin{equation}\n",
    "Z = U\\Sigma V^T\n",
    "\\end{equation}\n",
    "\n",
    "Where $U$ is an orthogonal $n\\times n$ matrix whose columns are the left eigenvectors of $Z$, $V$ is an orthogonal $k\\times k$ matrix whose columns are the right eigenvectors of $Z$, and $\\Sigma$ is a $n\\times k$ retangular diagonal matrix that represents the singular values of $Z$. (Since Z is real, these are all real.) Let us further define two matrices: the (symmetric) community-count $k\\times k$ matrix $\\Pi = Z^TZ$, the (symmetric) co-membership $n\\times n$ matrix $M=ZZ^T$. For these two, we can write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi &= Z^TZ = V\\Sigma^T U^TU\\Sigma V^T = V\\Sigma^T\\Sigma V^T = V\\Lambda_\\Pi V^T\\\\\n",
    "M &= ZZ^T = U\\Sigma V^TV\\Sigma^T U^T = U\\Sigma\\Sigma^TU^T = U\\Lambda_M U^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\Lambda_\\Pi$ and $\\Lambda_M$ are diagonal matrices representing the eigenvalues of their respective matrices, whose diagonal elements are nothing but square of the singular values of $Z$.\n",
    "\n",
    "### Adjacency Matrix\n",
    "\n",
    "For simplicity, consider the \"uncorrected\" expected adjacency matrix $A$. \n",
    "\\begin{equation}\n",
    "A=Z\\Psi Z^T\n",
    "\\end{equation}\n",
    "Now, corresponding to eigenvectors of $A$, we have the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "A v &= \\lambda v \\\\\n",
    "Z \\Psi Z^T v &= \\lambda v \\\\\n",
    "(Z^TZ)\\Psi Z^T v &= \\lambda Z^T v \\\\\n",
    "(\\Pi\\Psi)(Z^Tv) &= \\lambda (Z^T v) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the eigenvalues of $A$ correspond to eigenvalues of the product $\\Pi\\Psi$, and the eigenvectors of $\\Pi\\Psi$ correspond to $Z^T$ times the eigenvectors of $A$. More precisely, we can write the eigenvector $v$ of $A$ in terms of eigenvector $u$ of $\\Pi\\Psi$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Z^Tv &= u \\\\\n",
    "v &= (Z^T)^+u \\quad\\quad&\\text{where }X^+\\text{ refers to the Moore-Penrose pseudoinverse of }X \\\\\n",
    "v &= (V\\Sigma^TU^T)^+u\\\\\n",
    "v &= U(\\Sigma^T)^+V^Tu \\\\\n",
    "v &= U(\\Sigma^+)^TV^Tu \\quad\\quad&\\text{where }\\Sigma^+ \\text{ is simply replacing its non-zero diagonal elements by their inverse, followed by a transpose}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus we can easily solve for the spectrum of $A$ by solving for the spectrum of $\\Pi\\Psi$. A key thing to note here is that while $\\Psi$ decides the inter-community probability matrix, $\\Pi$ is a measure of the composition of our \"test\" population. It appears sensible then that the spectrum of $A$ should depend not only on how much communities discriminate against one another, but how much of each community exists in the society as well.\n",
    "\n",
    "We note that if the inverse of $\\Pi=Z^T Z$ exists (if community membership columns are linearly independent, which is likely the case) then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Z^+ &= \\Pi^{-1}Z^T \\\\\n",
    "&= V\\Lambda_\\Pi^{-1}V^T(U\\Sigma V^T)^T \\\\\n",
    "&= V\\Lambda_\\Pi^{-1}\\Sigma^TU^T \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Comparing this to what we would obtain by SVD of $Z$, we get that $\\Sigma^+ = \\Lambda_\\Pi^{-1}\\Sigma^T$. Therefore, $Z^+Z=Z^T(Z^T)^+=I_k$.\n",
    "\n",
    "### Laplacian Matrix\n",
    "\n",
    "For studying dynamics on a network, we would be interested in other matrices which are some kind of a derivative of the original Adjacency matrix. Given the analysis above, the spectrum of such matrices should also be deducible from the spectrum of some rank $k$ matrix.\n",
    "\n",
    "Let us consider the expected Laplacian matrix which governs diffusion processes on a graph, given by $L=D-\\hat{A}$, where $D=\\mathrm{diag}(\\hat{A}\\boldsymbol j)$ is a $n\\times n$ diagonal matrix capturing the expected degree of a node, $\\hat{A}$ is the expected adjacency matrix, $\\boldsymbol j = \\{1\\}^n$ is the unit vector. Because of how $L$ is defined, any self-loops in the graph are \"cancelled\" out, and indeed we can directly write $L$ in terms of $A$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L &= \\mathrm{diag}(A\\boldsymbol j) - A \\\\\n",
    "L &= Z(\\Pi^{-1}Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z\\Pi^{-1} - \\Psi)Z^T \\\\\n",
    "L &= Z(Z^+\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)(Z^+)^T - \\Psi)Z^T \\\\\n",
    "L &= Z\\mathcal{L}Z^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Therefore, we can represent the Laplacian $L$ in terms of a rank-reduced representation $\\mathcal{L}=Z^+\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)(Z^+)^T -\\Psi$, analogous to how we represented $A$ in terms of $\\Psi$. The eigenvalues and eigenvectors of $L$ thus analogously follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Eigenvalue Inequalities\n",
    "\n",
    "As noted above, we did not remove \"self-loops\" while estimating eigenvalues of the expected adjacency matrix. Ideally, we would like to do this, which involves subtracting the diagonal of the uncorrected adjacency matrix from itself. Moreover, to evaluate eigenvalues at the network level we need the eigenvalues of product of two matrices at the community-level. Although we can numerically perform both these operations of addition and multiplication, we might nevertheless be interested in closed-form estimates of the eigenvalues. For that purpose, we refer to Horn's inequalities that compare the eigenvalues of matrix sums or matrix products in terms of the eigenvalues of its constituents. (Although these results hold in general for any matrices, we are particularly interested in what happens when we add a diagonal matrix (to correct for the self-loops) or multiply a diagonal matrix (since under a weak assumption of single-memberships, $\\Pi$ is simply a positive diagonal matrix, and calculation of other varians of the graph Laplacian also involve some products using the diagonal degree matrix.)\n",
    "\n",
    "#### For Addition\n",
    "\n",
    "For real symmetric matrices $A, B, C=A+B$ with eigenvalues $\\{\\alpha_1 \\ge \\alpha_2 \\dots \\ge \\alpha_n\\},\\{\\beta_1 \\ge \\beta_2 \\dots \\ge \\beta_n\\},\\{\\gamma_1 \\ge \\gamma_2 \\dots \\ge \\gamma_n\\}$, we have the following [Weyl's inequalities](https://en.wikipedia.org/wiki/Weyl's_inequality):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{max}(\\{\\alpha_i+\\beta_j; i+j \\ge k+n\\}) \\le \\gamma_k \\le \\mathrm{min}(\\{\\alpha_i+\\beta_j; i+j \\le k+1\\})\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, from the property of sum of eigenvalues being equal to trace of the matrix, we have the following equality:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_i \\gamma_i = \\sum_i \\alpha_i + \\sum_i\\beta_i\n",
    "\\end{equation}\n",
    "\n",
    "#### For Multiplication\n",
    "\n",
    "For real symmetric matrices $A, B, C=AB$ with eigenvalues $\\{\\alpha_1 \\ge \\alpha_2 \\dots \\ge \\alpha_n\\},\\{\\beta_1 \\ge \\beta_2 \\dots \\ge \\beta_n\\},\\{\\gamma_1 \\ge \\gamma_2 \\dots \\ge \\gamma_n\\}$, they follow [exponentiated version of Weyl's inequalities](https://math.stackexchange.com/questions/20302/eigenvalues-of-product-of-a-matrix-and-a-diagonal-matrix)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{max}(\\{\\alpha_i\\beta_j; i+j \\ge k+n\\}) \\le \\gamma_k \\le \\mathrm{min}(\\{\\alpha_i\\beta_j; i+j \\le k+1\\})\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, from the property of product of eigenvalues being equal to determinant of the matrix, we have the following equality:\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_i \\gamma_i = \\prod_i \\alpha_i \\prod_i\\beta_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity Kernel as an SBM\n",
    "\n",
    "Let's recapitulate the design of the SBM matrix $\\Psi$ for the kind of social survey data we might have. We enforce a weak assumption of single-membership, that is a person can only belong to one community in the given categorical Blau dimension. Let $\\boldsymbol\\rho\\in[0,1]^k$ be a vector of \"proportion of friends\" parameters for the $k$ communities, that is $\\rho_i$ represents for the $i$th community the \"average\" proportion of their friends that are from within the community. Let $\\boldsymbol\\pi\\in(0,1)^k$ be the \"proportion of population\" parameters for the $k$ communities, that is $\\pi_i$ represents what proportion of the \"training\" population are people from community $i$ and thus $\\sum_i\\pi_i=1$. Let $\\omega$ be the mean edge density parameter. Then under the reasonable assumption that a \"source\" community discriminates against members of any other \"target\" community regardless of what that target community is (**\"equally discriminatory\" assumption**), we can succinctly write the symmetric SBM block matrix as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Psi_{ii} &= \\omega\\frac{\\rho_i}{\\pi_i} \\\\\n",
    "\\Psi_{ij} = \\Psi_{ji} &= \\frac{\\omega}{2}\\left(\\frac{1-\\rho_i}{1-\\pi_i} + \\frac{1-\\rho_j}{1-\\pi_j}\\right) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "Let $\\boldsymbol{\\hat{\\pi}}\\in[0,1]^k$ represent the distribution of communities in a \"testing\" population with assignments $Z$, that is $\\sum_i\\hat{\\pi_i}=1$. Clearly, $\\Pi=Z^TZ = n*\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}})$. As described above, the eigenvalues of the \"uncorrected\" expected adjacency matrix is given by the eigenvalues of\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi\\Psi = n*\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}})\\Psi\n",
    "\\end{equation}\n",
    "\n",
    "Let us explicitly write the values in this matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\Pi\\Psi)_{ii} &= n\\Psi_{ii}\\hat{\\pi_i} = \\omega n\\rho_i\\frac{\\hat{\\pi_i}}{\\pi_i} \\\\\n",
    "(\\Pi\\Psi)_{ij} &= n\\Psi_{ij}\\hat{\\pi_i} = \\frac{\\omega n}{2}\\left((1-\\rho_i)\\frac{\\hat{\\pi_i}}{1-\\pi_i} + (1-\\rho_j)\\frac{\\hat{\\pi_i}}{1-\\pi_j}\\right) \\\\\n",
    "(\\Pi\\Psi)_{ji} &= n\\Psi_{ji}\\hat{\\pi_j} = \\frac{\\omega n}{2}\\left((1-\\rho_j)\\frac{\\hat{\\pi_j}}{1-\\pi_j} + (1-\\rho_i)\\frac{\\hat{\\pi_j}}{1-\\pi_i}\\right) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Correspondingly for the Laplacian $L$, we need to estimate eigenvalues of $\\mathcal{L} = \\Pi^{-1}Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z\\Pi^{-1} - \\Psi$. We have the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j))_{xx} &= n\\sum_j\\Psi_{ij}\\hat{\\pi_j} \\quad\\quad\\text{where } Z_{xi}=1\\\\\n",
    "(Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z)_{ii}  &= n^2\\hat{\\pi_i}\\sum_j\\Psi_{ij}\\hat{\\pi_j} \\\\\n",
    "(\\Pi^{-1}Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z\\Pi^{-1})_{ii}  &= \\sum_j\\Psi_{ij}\\frac{\\hat{\\pi_j}}{\\hat{\\pi_i}} \\\\\n",
    "\\mathcal{L}_{ii} &= \\sum_{j\\neq i}\\Psi_{ij}\\frac{\\hat{\\pi_j}}{\\hat{\\pi_i}} \\\\\n",
    "\\mathcal{L}_{ij} = \\mathcal{L}_{ji} &= -\\Psi_{ij} = -\\Psi_{ji} \\\\\n",
    "\\mathcal{L} &= \\mathrm{diag}(\\Pi^{-1}\\Psi\\Pi\\boldsymbol{j}) -\\Psi\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus for $L$ we can write the eigenvalues of $\\Pi\\mathcal{L} = n*\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}})\\mathcal{L}$. Let us succinctly write the values of this matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\Pi\\mathcal{L})_{ii} &= n\\sum_{j\\neq i}\\Psi_{ij}\\hat{\\pi_j} \\\\\n",
    "(\\Pi\\mathcal{L})_{ij} &= -n\\Psi_{ij}\\hat{\\pi_i} \\\\\n",
    "(\\Pi\\mathcal{L})_{ji} &= -n\\Psi_{ji}\\hat{\\pi_j} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus the eigenvalues of $L$ are given by the eigenvalues of:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi\\mathcal{L} = \\mathrm{diag}(\\Psi\\Pi\\boldsymbol{j})-\\Pi\\Psi\n",
    "\\end{equation}\n",
    "\n",
    "### For Multiple Blau Dimensions\n",
    "\n",
    "Let us consider $m$ independent categorical Blau dimensions, each with $\\{k_1,k_2,\\dots k_m\\}$ number of communities. (The consideration of independence must be appropriate, for instance one might like to consider \"**income:** low, middle, high\" and \"**job:** unemployed, employed\" as a single \"**income X job**:\" categorical with $3\\times 2=6$ communities.) Correspondingly, we have their respective parameters $\\{\\boldsymbol\\rho_1\\in[0,1]^{k_1},\\boldsymbol\\rho_2\\in[0,1]^{k_2},\\dots \\boldsymbol\\rho_m\\in[0,1]^{k_m}\\}$, $\\{\\boldsymbol\\pi_1\\in(0,1)^{k_1},\\boldsymbol\\pi_2\\in(0,1)^{k_2},\\dots \\boldsymbol\\pi_m\\in(0,1)^{k_m}\\}$, and \"testing\" population proportions $\\{\\boldsymbol{\\hat{\\pi_1}}\\in[0,1]^{k_1},\\boldsymbol{\\hat{\\pi_2}}\\in[0,1]^{k_2},\\dots \\boldsymbol{\\hat{\\pi_m}}\\in[0,1]^{k_m}\\}$. Under the independence assumption, we can simply appropriately multiply dimension-specific probabilities from $\\Psi_y$ to obtain overall block matrix $\\Psi$.\n",
    "\n",
    "Let us define dimension-specific unsymmetric block matrices $\\overrightarrow{\\Psi_y}$ such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\overrightarrow{\\Psi_y})_{ii} &= \\frac{\\rho_{yi}}{\\pi_{yi}} \\\\\n",
    "(\\overrightarrow{\\Psi_y})_{ij} &= \\frac{1-\\rho_{yi}}{1-\\pi_{yi}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "For the overall SBM, we can define the $\\prod_yk_y\\times\\prod_yk_y$ sized block matrix as a Kroenecker product of dimension-specific block matrices.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\overrightarrow{\\Psi_\\otimes} &= \\omega \\bigotimes_{y=1}^m\\overrightarrow{\\Psi_y} \\\\\n",
    "\\Psi_{+\\otimes} &= \\frac{\\overrightarrow{\\Psi_\\otimes}+\\overrightarrow{\\Psi_\\otimes}^T}{2}\\\\\n",
    "\\Psi_{+\\otimes} &= \\frac{\\omega}{2}\\left(\\bigotimes_{y=1}^m\\overrightarrow{\\Psi_y} + \\bigotimes_{y=1}^m\\overrightarrow{\\Psi_y}^T\\right) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Note that what we obtain is not necessarily the same as what we'd have got on \"symmetrisizing\" before combining the dimensions as in $\\Psi_{\\otimes +} = \\omega\\bigotimes_{y=1}^m\\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2}$. The former is a kind of sum-of-products while the latter a product-of-sums. We can explicitly write values of these matrices as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Psi_{+\\otimes}[(r_1\\times r_2\\times \\dots r_m), (c_1\\times c_2\\times \\dots c_m)] &= \\frac{\\omega}{2}\\left(\\prod_{y=1}^m(\\overrightarrow{\\Psi_y})_{r_yc_y}+\\prod_{y=1}^m(\\overrightarrow{\\Psi_y})_{c_yr_y}\\right)\\\\\n",
    "\\Psi_{\\otimes +}[(r_1\\times r_2\\times \\dots r_m), (c_1\\times c_2\\times \\dots c_m)] &= \\omega\\prod_{y=1}^m\\frac{(\\overrightarrow{\\Psi_y})_{r_yc_y}+(\\overrightarrow{\\Psi_y})_{c_yr_y}}{2}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "One might argue that $\\Psi_{+\\otimes}$ is more appropriate, since it symmetrisizes only once, but $\\Psi_{\\otimes +}$ is likely to be more \"stable\". Let us pick this as $\\Psi=\\Psi_{\\otimes +}$.\n",
    "\n",
    "Along similar lines, we can define $\\boldsymbol{\\hat{\\pi}} = \\mathrm{vec}(\\dots\\mathrm{vec}(\\mathrm{vec}(\\boldsymbol{\\hat{\\pi}}_1\\boldsymbol{\\hat{\\pi}}_2^T)\\boldsymbol{\\hat{\\pi}}_3^T)\\dots\\boldsymbol{\\hat{\\pi}}_m^T)$. This leads to the definition of $\\Pi$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi = n\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y)\n",
    "\\end{equation}\n",
    "\n",
    "As before, $\\Psi$ and $\\Pi$ can be straightforwardly plugged into the formulae described in the previous section, for eigenvalues of $A$ and $L$. With a bit of algebraic manipulation, we can write them out quite succinctly. First, we use the mixed-product property of the Kroenecker product, i.e. $(A_{n\\times n}\\otimes B_{m\\times m})(C_{n\\times n}\\otimes D_{m\\times m}) = (AC\\otimes BD)$. Second, we note that $\\mathrm{diag}((A_{n\\times n}\\otimes B_{m\\times m})\\boldsymbol{j}_{nm}) = \\mathrm{diag}(A\\boldsymbol{j}_m) \\otimes \\mathrm{diag}(B\\boldsymbol{j}_n)$. Using these two, we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi\\Psi &= n\\omega\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\\\\n",
    "\\Pi\\mathcal{L} &= \\mathrm{diag}(\\Psi\\Pi\\boldsymbol{j}_{k_1\\times\\dots k_m}) - \\Pi\\Psi \\\\\n",
    "\\Pi\\mathcal{L} &= n\\omega\\bigotimes_{y=1}^m \\left\\{\\mathrm{diag}\\left(\\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\boldsymbol{j}_{k_y}\\right) - \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\right\\}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that the rank of these matrices is now $\\prod_y^mk_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Connectivity Kernel as an SBM\n",
    "\n",
    "Previously, we had assumed that a \"source\" community discriminates against members of any other \"target\" community regardless of what that target community is. Let us add another assumption to this one, that it is regardless of what the source community is either. That is, every community favours members of its own just as much as every other community, and every community discriminates against members of another community just as much as every other community (**\"identically equally discriminatory\"  (IED) assumption**). This effectively assumes that the training population is equally distributed amongst all communities, and reduces the number of learnable parameters of a single-dimension categorical Blau dimension model to just one $\\rho$ -- the \"forward\" kernel interpretation of Till's logistic connectivity kernel.\n",
    "\n",
    "Let $\\boldsymbol\\rho\\in[0,1]^m$ be a vector of \"proportion of friends\" parameters for the $m$ Blau dimensions, that is $\\rho_y$ represents for the $y$th Blau dimension the \"average\" proportion of friends that are from within a community. Let $\\boldsymbol\\pi\\in(0,1)^m$ be the \"proportion of population\" parameter, that is $n\\pi_y$ represents the average community size in the \"training\" population for the $y$th Blau dimension. Alternatively, we can have $\\boldsymbol k\\in\\{2,3,\\dots\\}^m$ represent the number of communities, and therefore $\\pi_y = \\frac{1}{k_y}$ (which are usually known in the survey design). This is a more intuitive parametrization under the IED assumption, so we use $\\boldsymbol k$ now instead of $\\boldsymbol\\pi$. Let $\\omega$ be the mean edge density parameter. We have the following for $\\Psi$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\Psi_y)_{ii} &= k_y\\rho_y \\\\\n",
    "(\\Psi_y)_{ij} = (\\Psi_y)_{ji} &= \\frac{1-\\rho_y}{1-1/k_y} \\\\\n",
    "\\Psi_{+\\otimes} = \\Psi_{\\otimes +} = \\Psi &= \\omega \\bigotimes_{y=1}^m\\Psi_y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "As before, for the expectation of \"uncorrected\" adjacency matrix (and expectation of Laplacian matrix), its eigenvalues are same as the eigenvalues of $\\Pi\\Psi$ (and of $\\Pi\\mathcal{L}$), which we can write as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi\\Psi &= n\\omega\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\Psi_y \\\\\n",
    "\\Pi\\mathcal{L} &= n\\omega\\bigotimes_{y=1}^m \\left\\{\\mathrm{diag}(\\Psi_y \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\boldsymbol{j}_{k_y}) - \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\Psi_y\\right\\}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We can solve for the above numerically. To obtain a closed form solution, we can use Weyl's inequalities. But to obtain exact closed-form solutions, we make another assumption that the **\"testing\" and \"training\" population is identically distributed**. (This assumption should be mostly valid, unless we wish to hypothesize about novel social setups wherein given $\\Psi$, the population proportions $\\Pi$ are tweaked to achieve certain \"desirable\" properties.) Consequently, since the training population is assumed to be equi-proportioned in IED, this makes the testing population also equiproportioned. This would imply that $(\\boldsymbol{\\hat{\\pi_y}})_i = \\pi_y = \\frac{1}{k_y}$. Therefore, we have the following simple relationships:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\widetilde{\\Psi}_y)_{ii} &=  \\rho_y \\\\\n",
    "(\\widetilde{\\Psi}_y)_{ij} = (\\widetilde{\\Psi}_y)_{ji} &=  \\frac{1-\\rho_y}{k_y-1} \\\\\n",
    "\\Pi\\Psi &= n\\omega \\bigotimes_{y=1}\\widetilde{\\Psi}_y \\\\\n",
    "\\Pi\\mathcal{L} &= n\\omega \\bigotimes_{y=1}\\mathrm{I} - \\widetilde{\\Psi}_y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Now for the Kroenecker product, we use the following fact. Suppose $A_{n\\times n}$ and $B_{m\\times m}$ are square matrices with eigenvalues $\\alpha_1, \\dots \\alpha_n$ and $\\beta_1, \\dots \\beta_m$ listed by multiplicity. Then the eigenvalues of $A\\otimes B$ are given by $\\alpha_i\\beta_j$. That is, we simply need to find eigenvalues separately for all $\\widetilde{\\Psi}_y$ and $\\mathrm{I}-\\widetilde{\\Psi}_y$ and perform their cross-product in order to compute eigenvalues of $A$ and $L$ respectively.\n",
    "\n",
    "Let $Y$ be a real symmetric matrix, and $\\alpha$, $\\beta$ be scalars. Thus $Y$ can be eigendecomposed into $Q_Y\\Lambda_Y Q_Y^T$ such that $Q_Y$ is an orthonormal matrix ($Q_YQ_Y^T=Q_Y^TQ_Y=I$) whose columns are the eigenvectors of $Y$, and $\\Lambda_Y$ is a diagonal matrix containing real eigenvalues of $Y$.\n",
    "\n",
    "#### Claim: For symmetric matrix $X=\\alpha\\mathrm{I}+\\beta Y$ we have eigenvalues $\\Lambda_X=\\alpha\\mathrm{I}+\\beta\\Lambda_Y$ and eigenvectors $Q_Y$\n",
    "\n",
    "Proof:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X &= \\alpha\\mathrm{I}+\\beta Y \\\\\n",
    "&= \\alpha Q_YQ_Y^T+\\beta Q_Y\\Lambda_YQ_Y^T \\\\\n",
    "&= Q_Y(\\alpha\\mathrm{I} + \\beta\\Lambda_Y)Q_Y^T \\\\\n",
    "&= Q_X\\Lambda_X Q_X^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, $\\Lambda_X = \\alpha\\mathrm{I} + \\beta\\Lambda_Y$ is a diagonal matrix, and $Q_X=Q_Y$ is orthogonal.\n",
    "\n",
    "We can use the above claim to easily find eigenvalues of $\\widetilde{\\Psi}_y$ and $\\mathrm{I}-\\widetilde{\\Psi}_y$. First, we write them as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\widetilde{\\Psi}_y &= \\frac{k_y\\rho_y-1}{k_y-1}\\mathrm{I} + \\frac{1-\\rho_y}{k_y-1}\\mathcal{I} \\\\\n",
    "\\mathrm{I} - \\widetilde{\\Psi}_y &= \\frac{k_y(1-\\rho_y)}{k_y-1}\\mathrm{I} - \\frac{1-\\rho_y}{k_y-1}\\mathcal{I} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Now we know eigenvalues of the identity matrix $\\mathrm{I}_n$ are $1$ with multiplicity $n$, and of the all-ones matrix $\\mathcal{I}_n$ are $n$ with multiplicity $1$ and $0$ with multiplicity $n-1$. Thus we have the following:\n",
    "\n",
    "1. Eigenvalues of $\\widetilde{\\Psi}_y$ are $1$ with multiplicity $1$ and $\\large\\frac{k_y\\rho_y-1}{k_y-1}$ with multiplicity $k_y-1$.\n",
    "2. Eigenvalues of $\\mathrm{I} - \\widetilde{\\Psi}_y$ are $0$ with multiplicity $1$ and $\\large\\frac{k_y(1-\\rho_y)}{k_y-1}$ with multiplicity $k_y-1$.\n",
    "3. Eigenvalues of $A$ are:\n",
    "    1. $n\\omega$ with multiplicity $1$\n",
    "    2. $n\\omega \\large\\frac{k_i\\rho_i-1}{k_i-1}$ with multiplicity $(k_i-1)$ evaluated over every $i\\in\\{1,2,\\dots m\\}$ Blau dimensions\n",
    "    3. $n\\omega \\large\\frac{k_i\\rho_i-1}{k_i-1}\\frac{k_j\\rho_j-1}{k_j-1}$ with multiplicity $(k_i-1)(k_j-1)$ evaluated over each $i,j\\in\\{1,2,\\dots m\\}; i\\neq j$ pair of Blau dimensions\n",
    "    4. $\\dots$\n",
    "    5. $n\\omega \\large\\prod_{y=1}^m\\frac{k_y\\rho_y-1}{k_y-1}$ with multiplicity $\\prod_{y=1}^m(k_y-1)$ evaluated over all Blau dimensions\n",
    "4. Eigenvalues of $L$ are:\n",
    "    1. $0$ with multiplicity $\\prod_{y=1}^mk_y - \\prod_{y=1}^m(k_y-1)$\n",
    "    5. $n\\omega \\large\\prod_{y=1}^m\\frac{k_y(1-\\rho_y)}{k_y-1}$ with multiplicity $\\prod_{y=1}^m(k_y-1)$ evaluated over all Blau dimensions\n",
    "\n",
    "#### Eigenvalues of $A$\n",
    "1. Note that for all $y$, since $0 \\le \\rho_y \\le 1$ and $k_y\\ge 2$, $-1 \\le\\frac{k_y\\rho_y-1}{k_y-1} \\le 1$. That is, the eigenvalues for $A$ are listed according to monotonically non-increasing absolute values.\n",
    "2. The largest eigenvalue $\\lambda_1$ of $A$ is known to correspond to average degree of the graph. Clearly, $\\lambda_1=n\\omega$ is the mean degree of every node in an $n$ sized network. Moreover, for d-regular graphs, all eigenvalues lie between $[-d,d]$, which is also what we note here.\n",
    "3. Recall (from the previous notebook) that the $y$th Blau dimension is heterophilous if $k_y\\rho_y <1$, ambiphilous if $k_y\\rho_y =1$ and homophilous if $k_y\\rho_y >1$. In this light, a homophilous (heterophilous) Blau dimension contributes towards a positive (negative) eigenvalue, and ambiphilous to $0$ eigenvalues.\n",
    "\n",
    "#### Eigenvalues of $L$\n",
    "1. Note that for all $y$, since $0 \\le \\rho_y \\le 1$ and $k_y\\ge 2$, $0 \\le\\frac{k_y(1-\\rho_y)}{k_y-1}$.\n",
    "2. Since $L$ is a positive-semidefinite matrix (it is diagonally dominant), it has non-negative eigenvalues, which is also what we note here. The smallest eigenvalue of $L$ is $0$.\n",
    "3. $L$ has essentially only two eigenvalues, one is $0$ and the other is $n\\omega \\large\\prod_{y=1}^m\\frac{k_y(1-\\rho_y)}{k_y-1}$.\n",
    "4. The number of \"connected\" components correspond to the algebraic multiplicity of $0$, which is given by $\\prod_{y=1}^mk_y - \\prod_{y=1}^m(k_y-1)$. One might have expected the number of connected components to simply be $\\prod_y k_y$, since there are $k_y$ communities in $y$th dimension and dimensions are assumed to be independent. Clearly, what this problem ends up considering is an \"excess-of-one\" measure of number of connected components. That is, the number of connected components is exacly equal to the reduction in *overall* number of communities when the number of communities of *every* Blau dimension is reduced by $1$. Using the inclusion-exclusion principle, it appears that with every additive dimension the network breaks up exponentially into \"connected\" components.\n",
    "    1. For $m=1$, this gives exactly $1$ connected component.\n",
    "    2. For $m=2$, this gives $k_1+k_2-1$ connected components.\n",
    "    3. For $m=3$, this gives $k_1k_2+k_2k_3+k_3k_1-k_1-k_2-k_3+1$ connected components.\n",
    "    4. In general, this gives $\\sum_{i=1}^m (-1)^{m-i} \\left\\{\\sum_{1\\le y_1\\le y_2 \\dots y_i\\le m}\\prod_{j=1}^ik_{y_j}\\right\\}$ connected components.\n",
    "5. The second-smallest eigenvalue of $L$ corresponds to the algebraic connectivity of the graph. Clearly, if $m>1$, this value is $0$, and the graph is disconnected. If $m=1$ (one connected component), then this value is given by $n\\omega \\large\\frac{1-\\rho}{1-1/k}$. Clearly, algebraic connectivity is:\n",
    "    1. $\\propto n$, the number of people\n",
    "    2. $\\propto \\omega$, the mean edge density\n",
    "    3. inversely proportional to $k$, the number of components\n",
    "    4. inversely proportional to $\\rho$, the proportion of friends of same community\n",
    "    5. less (more) than the average degree $n\\omega$ for a homophilous (heterophilous) kernel\n",
    "6. The smallest non-zero eigenvalue of $L$ is called the spectral gap. This is essentially $\\lambda_\\Delta = n\\omega \\large\\prod_{y=1}^m\\frac{k_y(1-\\rho_y)}{k_y-1}$. If the $y$th Blau dimension is homophilous, then $\\frac{k_y(1-\\rho_y)}{k_y-1} < 1$, if heterophilous, then $\\frac{k_y(1-\\rho_y)}{k_y-1} > 1$, and if ambiphilous then $\\frac{k_y(1-\\rho_y)}{k_y-1} = 1$. In this light, a homophilous (heterophilous) dimension weighs down (up) the spectral gap of $L$, while an ambiphilous one leaves it unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Eigenvalues of $A$ for Connectedness\n",
    "\n",
    "The above closed-form solutions on the expectation of (uncorrected) $A$ can be further used to derive some interesting results about the social network.\n",
    "\n",
    "#### Cheeger's Inequalities for Expander Graphs\n",
    "\n",
    "Since the \"expected\" degree of the graph turns out to be the same for every node, this is equivalent to a d-regular graph where $d=\\omega n$. We can thus apply techniques from theory of expander graphs. Expander graphs are sparse graphs with strong connectivities, and thus could be relevant to robust social networks.\n",
    "\n",
    "Let $S$ be a subset of vertices from the graph $A$. Let $\\partial S$ represent the edge boundary of $S$, that is the number of edges that \"cross\" the boundary of this set. The isoparametric number or Cheeger constant $h(A)$ is defined as the minimum of $\\frac{|\\partial S|}{|S|}$ when evaluated over all such $S$ such that $|S|\\le n/2$. Clearly, this constant can be seen as some notion of a \"bottleneck\" in the graph. (The smaller it is, the more bottlenecked the network is.) For expander graphs, this constant must be high.\n",
    "\n",
    "Given the interpretation of $A$ as a regular graph, we can use Cheeger's inequaities which relates eigenvalues of $A$ to bounds on $h(A)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\lambda_1-\\lambda_2) \\le h(A) \\le \\sqrt{2\\lambda_1(\\lambda_1-\\lambda_2)}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the difference in first two eigenvalues of $A$ is essentially the spectal gap, which is equal to the smallest non-zero eigenvalue of $L$. (Interestingly, this holds only when we have $1$ Blau dimension.) Now for our simplified connectivity kernel, the first eigenvalue of $A$, $\\lambda_1=n\\omega$, and the second eigenvalue of $A$, $\\lambda_2=n\\omega*\\mathrm{max}\\left\\{\\left|\\frac{k_y\\rho_y-1}{k_y-1}\\right|; \\forall y\\in\\{1,2,\\dots m\\}\\right\\}$.\n",
    "\n",
    "To find the maximum of the argument, we observe the following for two homophilous dimensions $i$ and $j$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\frac{k_i\\rho_i-1}{k_i-1} \\ge \\frac{k_j\\rho_j-1}{k_j-1} \\\\\n",
    "\\implies & k_ik_j(\\rho_i-\\rho_j) \\ge k_j(1-\\rho_j) + k_i(\\rho_i-1) \\\\\n",
    "\\implies & \\text{if } \\rho_i=\\rho_j \\text{ then }  k_i \\ge k_j \\\\\n",
    "\\implies & \\text{if } k_i=k_j \\text{ then }  \\rho_i \\ge \\rho_j \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, $i$ is the *even more* \"homophilous\" dimension. Similarly, were both $i$ and $j$ heterophilous, then $i$ is the *even more* \"heterophilous\" dimensions. When one of them is homophilous and the other heterophilous, then the more \"extremal\" dimension maximizes the argument. Let $\\hat{y}$ be that \"most anti-ambiphilous\" Blau dimension that maximizes the argument above. For notational simplicity, let $\\rho_{\\hat{y}} = \\rho, k_{\\hat{y}} = k$. Then we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda_2 = n\\omega\\left|\\frac{k\\rho-1}{k-1}\\right|\n",
    "\\end{equation}\n",
    "\n",
    "Now from Cheeger's inequalities, if $\\hat{y}$ is homophilous:\n",
    "\n",
    "\\begin{equation}\n",
    "n\\omega\\frac{1-\\rho}{1-1/k} \\le h(A) \\le n\\omega\\sqrt{2\\frac{1-\\rho}{1-1/k}}\n",
    "\\end{equation}\n",
    "\n",
    "If $\\hat{y}$ is heterophilous (although it is quite unlikely in a social scenario):\n",
    "\n",
    "\\begin{equation}\n",
    "n\\omega\\frac{1+\\rho-2/k}{1-1/k} \\le h(A) \\le n\\omega\\sqrt{2\\frac{1+\\rho-2/k}{1-1/k}}\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, a homophilous \"extremal\" or a heterophilous \"extremal\" dimension decrease (increase) both bounds by having higher and lower proportion of friends similar to them respectively, thus increasing (decreasing) the bottleneckedness.\n",
    "\n",
    "#### Ramanujan Graphs: Excellent Expanders\n",
    "\n",
    "If the upper bound can be asymptotically increased, that is if the spectral gap is somehow as large as it possibly can be, then we can obtain an excellent expander graph (sparse yet highly connected). This is referred to as a Ramanujan graph. For a d-regular graph, let $\\lambda_R = \\mathrm{max}_{|\\lambda_i|<d} |\\lambda_i|$. Then $\\lambda_R$ is shown to satisfy $\\lambda_R \\ge 2\\sqrt{d-1} - o(1)$. Thus, a d-regular graph $A$ is a Ramanujan Graph if $\\lambda_R \\le 2\\sqrt{d-1}$.\n",
    "\n",
    "As before, let $\\hat{y}$ be the most anti-ambiphilous dimension, corresponding to $\\lambda_R$. Let $\\delta = \\frac{2\\sqrt{d-1}}{d}$. Then for our simple SBM kernel, we can derive the following conditions for being a Ramanujan Graph.\n",
    "\n",
    "If $\\hat{y}$ is an extremely homophilous dimension:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho \\le \\frac{1}{k} (1+\\delta(k-1))\n",
    "\\end{equation}\n",
    "\n",
    "If $\\hat{y}$ is an extremely heterophilous dimension:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho \\ge \\frac{1}{k} (1-\\delta(k-1))\n",
    "\\end{equation}\n",
    "\n",
    "This condition shows an \"acceptable\" limit of $\\pm\\delta(1-1/k)$ on $\\rho$ of the most extremely anti-ambiphilous dimension, so as to keep the network as an excellent expander. Clearly, $0 \\le \\rho \\le 1$, thus the first constraint is needed only when $\\delta < 1$, and the latter only when $\\delta < \\frac{1}{k-1}$. Now for $d>2$, we have $\\delta < 1$ but $\\delta > \\frac{1}{k-1} \\forall k\\ge 2$.\n",
    "\n",
    "Therefore, if the most anti-ambiphilous dimension is heterophilous, then the graph is always a Ramanujan Graph. However if it is homophilous, which is the more likely case for social networks anyway, then the graph is a Ramanujan Graph only upto a certain limit on homophily: if $\\rho \\le \\frac{1}{k} (1+\\delta(k-1))$. Moreover, in the asymptotic limit of this remaining constraint, as $d \\rightarrow \\infty, \\delta \\rightarrow 0$, and the RHS  $\\rightarrow 1/k$. Which means that for a fairly large-degree network the condition of being a Ramanujan graph approaches the condition of heterophily. **In essence, more heterophily can never be hurtful to well-connectedness.**\n",
    "\n",
    "Let's go back to the remaining constraint. Quite often, $k$ is a large number, and more importantly \"uncontrollable\" for people as social agents. Thus if we let $k\\rightarrow \\infty$, we can obtain a safe upper limit on $\\rho$ that will ensure the graph to be an excellent expander. This transforms the constraint into :\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\rho & \\le\\delta \\\\\n",
    "\\rho d & \\le 2\\sqrt{d-1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\rho$ is a representation of the proportion of friends, while $d$ of the number of friends, we obtain a very simple and \"controllable\" mechanism to ensure that the graph is an excellent expander: **have no more than twice the square root of number of friends over one be similar to you** in the most homophilous Blau dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
