{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Stochastic Block Model Approach to Connectivity Kernels\n",
    "\n",
    "Previously, we saw how in the case of a categorical Blau space, the logistic connectivity kernel can be better represented as a stochastic block model (SBM), wherein nodes are members of a certain community $i \\in \\{1,2\\dots k\\}$ in the given Blau dimension, whose members form friendships according to a symmetric community-level probability matrix $\\Psi=\\{\\psi_{ij}\\}_{i,j=1}^n$.\n",
    "\n",
    "Let us consider a 1-dimensional categorical Blau space with $n$ people in $k$ communities. For each person $x_i \\in \\mathcal{X}$, we can write a corresponding membership vector $z_i \\in \\{0,1\\}^k$ containing exactly one $1$. (We could potentially relax this to mixed-membership vectors $z_i \\in [0,1]^k$ that sum to $1$.) Correspondingly, consider the assignment matrix $Z \\in \\{0,1\\}^{n\\times k}$. Then the stoachastic block model essentially suggests.\n",
    "\n",
    "$$A \\sim\\mathrm{Bernoulli}(Z\\Psi Z^T)$$\n",
    "\n",
    "Note that this can lead to self-loops, which we would not like to have in the graph. Let us write instead for the \"simple\" graph $\\hat{A}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbb{E}[A|Z,\\Psi] &= Z\\Psi Z^T \\\\\n",
    "\\mathbb{E}[\\hat{A}|A] &= \\mathbb{E}[A|Z,\\Psi] - \\mathrm{diag}(\\mathbb{E}[A|Z,\\Psi])\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Things to Note\n",
    "\n",
    "1. Clearly, the $\\mathrm{rank}(A) \\le \\mathrm{min}(\\mathrm{rank}(Z), \\mathrm{rank}(\\Psi))$, which implies $\\mathrm{rank}(A) \\le k$. This satisfies our intuition that the adjacency matrix of an SBM model probably has a basis of the order of number of communities, and not number of people. This means that we can effectively reduce problems concerning with the adjacency matrix down to the level of the probability matrix instead, as we attempt to do below.\n",
    "2. Since this is a probabilistic model, we will mostly talk of the expectation of the adjacency matrix below.\n",
    "\n",
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "Say we have somehow estimated the SBM matrix $\\Psi$, and are interested in the eigenspectrum of the corresponding adjacency matrix (more importantly perhaps the Laplacian matrix since it captures diffusion on the network). The above note suggests we should be able to do this at the level of $\\Psi$. Since $\\Psi$ and $A$ are both symmetric (we assume undirected networks), we already know they have real eigenvalues, and they can be eigendecomposed into a form given by $X=Q_{X}\\Lambda_{X}Q_{X}^T$ such that $\\Lambda_{X}$ is a diagonal matrix containing the eigenvalues of $X$, and $Q$ is an orthogonal matrix whose columns contain the corresponding eigenvectors. For notational convenience, we'll indicate $\\mathbb{E}[A|Z,\\Psi]$ simply as $A = Z\\Psi Z^T$. Now, corresponding to eigenvectors of $A$, we have the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "A v &= \\lambda v \\\\\n",
    "Z \\Psi Z^T v &= \\lambda v \\\\\n",
    "(Z^TZ)\\Psi Z^T v &= \\lambda Z^T v \\\\\n",
    "(D_Z\\Psi)(Z^Tv) &= \\lambda (Z^T v) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the eigenvalues of $A$ correspond to eigenvalues of the product $D_Z\\Psi$, and the eigenvectors of $A$ correspond to $M_ZZ$ times the eigenvectors of the product $D_Z\\Psi$. Here, $D_Z$ refers to the $k\\times k$ community-count matrix $Z^TZ$ and $M_Z$ refers to inverse of the $n\\times n$ co-membership matrix $ZZ^T$. Clearly, if mixed-membership is not allowed, then $D_Z$ is a diagonal matrix. Moreover, if communities are equally represented, then $D_Z = (n/k)\\boldsymbol{\\mathrm{I}}$. Thus eigenvalues of $A$ are simply eigenvalues of $\\Psi$ scaled up by $n/k$.\n",
    "\n",
    "Alternatively, one can also write $\\Psi = D_Z^{-1}Z^TAZD_Z^{-1}$. Now, corresponding to eigenvectors of $\\Psi$, we have the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Psi v &= \\lambda v \\\\\n",
    "D_Z^{-1}Z^TAZD_Z^{-1} v &= \\lambda v \\\\\n",
    "ZD_Z^{-2}Z^TAZD_Z^{-1} v &= \\lambda ZD_Z^{-1}v \\\\\n",
    "(Q_ZA)(ZD_Z^{-1} v) &= \\lambda (ZD_Z^{-1}v) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the eigenvalues of $\\Psi$ correspond to eigenvalues of the product $Q_ZA$, and the eigenvectors of $\\Psi$ correspond to $Z^T$ times the eigenvectors of the product $Q_ZA$. Here, $Q_Z$ refers to the $n\\times n$ inverse-quadratically weighted co-membership matrix $ZD_Z^{-2}Z^T$.\n",
    "\n",
    "Clearly, if mixed-membership is not allowed, then $Q_Z$ is a block-diagonal matrix. Moreover, if communities are equally represented, then $Q_Z = (k/n)^2\\boldsymbol{\\mathrm{I}}\\otimes\\mathcal{I}_{n/k\\times n/k}$ and $A = \\Psi\\otimes\\mathcal{I}_{n/k\\times n/k}$, where $\\mathcal{I}_{n/k\\times n/k}$ is an all-ones or unit matrix of size $n/k\\times n/k$, assuming $n$ is a multiple of $k$ (since communities have same number of members). Then by using the mixed-product property of Kroenecker product we have $Q_ZA = (k/n)^2\\Psi\\otimes\\mathcal{I}_{n/k\\times n/k}^2 = (k/n)\\Psi\\otimes\\mathcal{I}_{n/k\\times n/k}$. Now, for the Kroenecker product $A\\otimes B$, let $\\lambda_1,\\dots, Î»_n$ be the eigenvalues of $A$ and $\\mu_1,\\dots, \\mu_m$ be those of $B$ (listed according to multiplicity), then the eigenvalues of $A\\otimes B$ are given by $\\lambda_i\\mu_j$ for $i\\in\\{1,\\dots n\\},j\\in\\{1,\\dots m\\}$. Now $\\mathcal{I}_{n/k\\times n/k}$ has eigenvalue $n/k$ with multiplicity 1 and rest $0$. Eigenvalues of $Q_ZA = (k/n)\\Psi\\otimes\\mathcal{I}_{n/k\\times n/k}$ are thus eigenvalues of $\\Psi$, scaled up by $n/k$ (due to the unit matrix), then scaled down by $k/n$. Hence, we obtain eigenvaues of $Q_ZA$ simply as eigenvalues of $\\Psi$, as expected.\n",
    "\n",
    "### For the Laplacian Matrix\n",
    "\n",
    "Similarly, it is straightforward to see how eigenvalues of $\\Psi$ get propagated to the Laplacian matrix. We define $L = D - \\hat{A}$, where D is the degree matrix of adjacency matrix $\\hat{A}$. (Since this is a probabilistic model, these are actually *expected* degree and Laplacian matrix.) Although we've been basing the discussion above on \"uncorrected\" $A$, we can straightforwardly define the Laplacian using just $A$ as $$L = \\mathrm{diag}(A\\boldsymbol{j}) - A$$ where $\\boldsymbol{j}$ is the unit vector. Pre and most multiplying by $Z^T$ and $Z$, we obtain $$Z^TLZ = Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol{j})Z - D_Z\\Psi D_Z $$ Let $\\mathcal{L} = W_Z - D_Z\\Psi D_Z$ where $W_Z =Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol{j})Z $ is a $k\\times k$ matrix that refers to the member-degree-weighted community-count matrix. Analogous to the relationship between $A$ and $\\Psi$, we have $$L = M_ZZ\\mathcal{L}Z^TM_Z$$. Since $M_Z$ is symmetric, we can write this as $$L = (M_ZZ)\\mathcal{L}(M_ZZ)^T$$\n",
    "\n",
    "We can now analogously repeat the analysis we did above.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L v &= \\lambda v \\\\\n",
    "M_ZZ \\mathcal{L} Z^TM_Z v &= \\lambda v \\\\\n",
    "Z^TM_Z^2Z \\mathcal{L} Z^TM_Z v &= \\lambda Z^TM_Z v \\\\\n",
    "(R_Z\\mathcal{L})(Z^TM_Zv) &= \\lambda (Z^TM_Z v) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Here $R_Z$ refers to the $k\\times k$ inverse-quadraticaly-weighted community-count matrix $Z^TM_Z^2Z$. The eigenvalues of $L$ correspond to eigenvalues of the product $R_Z\\mathcal{L}$, and the eigenvectors of $L$ correspond to $Z$ times the eigenvectors of the product $R_Z\\mathcal{L}$.\n",
    "\n",
    "Alternatively, we can express $\\mathcal{L} =Z^TLZ$ and figure that eigenvalues of $\\mathcal{L}$ correspond to eigenvalues of the product $M_Z^{-1}L$, and the eigenvectors of $\\mathcal{L}$ correspond to $D_Z^{-1}Z^T$ times the eigenvectors of the product $M_Z^{-1}L$. Clearly, if mixed-membership is not allowed, then $M_Z^{-1}$ is a block diagonal matrix. Moreover, if communities are equally represented, then using properties of the Kroenecker product it can be shown that eigenvalues of $L$ are simply eigenvalues of $\\mathcal{L}$ scaled down by $(k/n)^2$. Further, the expression of $\\mathcal{L}$ itself gets simplified to $$\\mathcal{L} = (n/k)^2(\\mathrm{diag}(\\Psi\\boldsymbol{j}) - \\Psi)$$ This leads to eigenvalues of $L$ being exactly the same as eigenvalues of $L_\\psi=\\mathrm{diag}(\\Psi\\boldsymbol{j}) - \\Psi$, where $L_\\Psi$ can be called the Laplacian of the SBM's probability matrix.\n",
    "\n",
    "## Back to a \"Simpler\" SBM\n",
    "\n",
    "The discussion above is for any general SBM model. To keep things simple, let us consider the SBM model described earlier, wherein essentially $\\Psi_{ii}=k\\rho\\omega$ and $\\Psi_{ij}=\\frac{1-\\rho}{1-1/k}\\omega$. (Recall that $k$ is number of communities, $\\rho$ is proportion of friends in same community, and $\\omega$ is the average edge density.) \n",
    "\n",
    "### Eigenvalues and Eigenvectors of Converse Connectivity Kernel\n",
    "Let us figure the eigenvalues and eigenvectors of the network described by this model. Let $Y$ be a symmetric matrix, and $\\alpha$, $\\beta$ be scalars.\n",
    "\n",
    "#### Claim: For symmetric matrix $X=\\alpha\\boldsymbol{\\mathrm{I}}+\\beta Y$ we have eigenvalues $\\Lambda_X=\\alpha\\boldsymbol{\\mathrm{I}}+\\beta\\Lambda_Y$ and eigenvectors $Q_Y$\n",
    "\n",
    "Proof:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X &= \\alpha\\boldsymbol{\\mathrm{I}}+\\beta Y \\\\\n",
    "&= \\alpha Q_YQ_Y^T+\\beta Q_Y\\Lambda_YQ_Y^T \\\\\n",
    "&= Q_Y(\\alpha\\boldsymbol{\\mathrm{I}} + \\beta\\Lambda_Y)Q_Y^T \\\\\n",
    "&= Q_X\\Lambda_X Q_X^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, $\\Lambda_X$ is a diagonal matrix, and $Q_Y$ is orthogonal.\n",
    "\n",
    "Now for the SBM above we can write the probability matrix as $$\\Psi = \\omega\\left(\\frac{\\rho-\\frac{1}{k}}{\\frac{1}{k}\\left(1-\\frac{1}{k}\\right)}\\boldsymbol{\\mathrm{I}}+\\frac{1-\\rho}{1-\\frac{1}{k}}\\mathcal{I}\\right)$$\n",
    "\n",
    "Additionally for $\\mathcal{I}$ the eigenvalues are $k$ with multiplicity 1 and $0$ with multiplicity $k-1$. Using the claim above, eigenvalues of $\\Psi$ are given by $\\omega k$ with multiplicity $1$ and $\\omega\\frac{\\rho-\\frac{1}{k}}{\\frac{1}{k}\\left(1-\\frac{1}{k}\\right)}$ with multiplicity $k-1$.\n",
    "\n",
    "Let us assume that the \"test\" population, for whom we are evaluating the adjacency matrix, is also equiproportioned. Then we know eigenvalues of $A$ correspond to $n/k$ times eigenvalues of $\\Psi$: eigenvalues of $A$ are given by $\\omega n$ with multiplicity $1$ and $\\omega n \\frac{\\rho-\\frac{1}{k}}{1-\\frac{1}{k}}$ with multiplicity $k-1$.\n",
    "\n",
    "For $L_\\Psi$, under equiproportions, we get\n",
    "$$L_\\Psi = \\omega\\left(\\frac{1-\\rho}{\\frac{1}{k}\\left(1-\\frac{1}{k}\\right)}\\boldsymbol{\\mathrm{I}}-\\frac{1-\\rho}{1-\\frac{1}{k}}\\mathcal{I}\\right)$$\n",
    "\n",
    "Reusing the claim above, eigenvalues of $L_\\Psi$ and thus $L$ are given by $0$ with multiplicity $1$ and $\\omega\\frac{1-\\rho}{\\frac{1}{k}\\left(1-\\frac{1}{k}\\right)}$ with multiplicity $k-1$.\n",
    "\n",
    "#### Things To Note\n",
    "\n",
    "1. For $A$, the largest eigenvalue $\\mu_1$ is known to be related to the average degree of nodes $d_{avg}\\le\\mu_1\\le d_{max}$. We obtain this value as $\\omega n$, which is purely dependent only on the mean edge density $\\omega$, and is indeed exactly the the expected degree of a node (recall that $A$ can have self loops).\n",
    "2. The other eigenvalues of $A$ ($\\mu_2=\\dots...\\mu_k$) are positive for a homophilous kernel ($\\rho > 1/k$) and negative for a heterophilous kernel ($\\rho < 1/k$), and exactly zero for an ambiphilous kernel. But it can be shown that $\\forall\\rho\\in(0,1),\\forall k>1$, $|\\mu_k|<\\omega n$.\n",
    "3. It is [known](http://www.cs.yale.edu/homes/spielman/561/2012/lect03-12.pdf) if $A$ is a connected graph, then $\\mu_k=-\\mu_1$ if and only if A is bipartite. Clearly, a large negative $\\mu_k$ is only possible for a highly heterophilous kernel, which would mean people of a community almost always never connect to one another, which naturally lends a \"k-partite\" structure to the graph.\n",
    "4. Clearly, $0$ is always an eigenvalue of the Laplacian $L$.\n",
    "5. The spectral gap is same as the algebraic connectivity here and given by $\\omega\\frac{1-\\rho}{\\frac{1}{k}\\left(1-\\frac{1}{k}\\right)}$. Clearly, it is independent of the number of nodes in the graph, directly proportionate to mean edge density, and is always positive. Given a $k$, it is inversely proportionate to $\\rho$. Given a $\\rho$, it is directly proportionate to $k$.\n",
    "6. For an ambiphilous kernel ($\\rho=1/k$), the algebraic connectivity is equal to $\\omega k$. For a homophilous (heterophilous) kernel, it is less (more) than $\\omega k$. Note that since the kernel is homophilous, people do not discriminate **at all**. This might make us intuit that the algebraic connectivity should not depend on $k$ at all. However on second thoughts, $k$ is essentially rank of the adjacency matrix. And thus if $k$ is high, but people still remain \"ambiphilous\", then they are indeed more \"connected\"(?)\n",
    "\n",
    "#### Aside on Expander Graphs\n",
    "\n",
    "Since the \"expected\" degree of the simple graph turns out to be the same for every node, this is equivalent to a d-regular graph where $d=\\omega n$. We can thus apply techniques from theory of expander graphs. Expander graphs are sparse graphs with strong connectivities, and thus could be relevant to robust social networks.\n",
    "\n",
    "Let $S$ be a subset of vertices from the graph $A$. Let $\\partial S$ represent the edge boundary of $S$, that is the number of edges that \"cross\" the boundary of this set. The isoparametric number or Cheeger constant $h(A)$ is defined as the minimum of $\\frac{|\\partial S|}{|S|}$ when evaluated over all such $S$ such that $|S|\\le n/2$. Clearly, this consant can be seen as some notion of a \"bottleneck\" in the graph. (The smaller it is, the more bottlenecked the network is.) For expander graphs, this constant must be high.\n",
    "\n",
    "Given the interpretation of $A$ as a regular graph, we can use Cheeger's inequaities which relates eigenvalues of $A$ to bounds on $h(A)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\lambda_1-\\lambda_2) \\le h(A) \\le \\sqrt{2\\lambda_1(\\lambda_1-\\lambda_2)}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the difference in first two eigenvalues of $A$ is essentially the spectal gap, which is equal to the smallest non-zero eigenvalue of $L$. For our simple SBM setting wih equiproportion test population, we can describe these bounds as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\omega k^2(1-\\rho)}{2(k-1)} \\le h(A) \\le \\omega k\\sqrt{\\frac{2n(1-\\rho)}{k-1}}\n",
    "\\end{equation}\n",
    "\n",
    "Assuming a homophilous kernel ($\\rho=1/k$), and writing number of members per community as $m=n/k$, and average degree as $d=\\omega n$:\n",
    "\n",
    "\\begin{equation}\n",
    "d \\frac{1}{2} \\le h(A) \\le d\\sqrt{2m}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, despite a \"non-discriminatory\" kernel, the larger the number of members of each community, the less bottlenecked the network can potentially be. Clearly, a homophilous kernel lowers both bounds, while a heterophilous one increases them.\n",
    "\n",
    "#### Aside on Ramanujan Graphs\n",
    "\n",
    "If the upper bound can be asymptotically increased, that is if the spectral gap is somehow as large as it possibly can be then we can obtain an excellent expander graph (sparse yet highly connected). This is referred to as a Ramanujan graph. The spectral gaps $\\lambda_\\delta$ are already shown to satisfy $\\lambda_\\Delta \\ge 2\\sqrt{d-1} - o(1)$. Thus, a d-regular graph $A$ is a Ramanujan Graph if the maximum spectral gap satisfies $\\lambda_\\Delta \\le 2\\sqrt{d-1}$.\n",
    "\n",
    "Let $\\delta = \\frac{\\sqrt{d-1}}{d}$. Then for the simple SBM with equiproportioned test population we can derive the condition under which the graph is likely a Ramanajun graph:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho \\le \\frac{1}{k} (1+2\\delta(k-1))\n",
    "\\end{equation}\n",
    "\n",
    "This condition shows an \"acceptable\" limit on homophily to keep the network as an excellent expander. That is, $\\rho$ can be larger than $1/k$, but it must be smaller than the LHS of the above constraint. Clearly, $\\rho \\le 1$, thus this constraint is needed only when $\\delta < 1/2$, that is when $d > 2$, which will almost always be the case. In the asymptotic limit, as $d \\rightarrow \\infty$, the RHS approaches $1/k$, which means that for a respectively large degree network the condition of being a Ramanujan graph is the same as the condition of heterophily.\n",
    "\n",
    "## Extending to Multiple Blau Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE; incorrect \n",
    "\n",
    "Let $Y$ be a symmetric matrix, and $D$ a positive diagonal matrix.\n",
    "\n",
    "#### Claim 1: For symmetric matrix $X=DYD$ we have $\\Lambda_X = D^2\\Lambda_Y$ and $Q_X = DQ_YD^{-1}$\n",
    "\n",
    "Proof:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X &= DYD \\\\\n",
    "&=DQ_Y\\Lambda_YQ_Y^TD \\\\\n",
    "&=DQ_YD^{-1}D\\Lambda_YDD^{-1}Q_Y^TD \\\\\n",
    "&=(DQ_YD^{-1})D^2\\Lambda_Y(DQ_YD^{-1})^T \\\\\n",
    "&=Q_X\\Lambda_XQ_X^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, $\\Lambda_X$ is a diagonal matrix, but $Q_X$ is NOT orthogonal :(\n",
    "\n",
    "#### Claim 2: For symmetric matrix $X=D\\pm Y$ we have $\\Lambda_X = D\\pm \\Lambda_Y$ and $Q_X = Q_Y$\n",
    "\n",
    "Proof:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X &= D \\pm Y \\\\\n",
    "&=D^{1/2}D^{1/2} \\pm D^{1/2}D^{-1/2}YD^{-1/2}D^{1/2} \\\\\n",
    "&=D^{1/2}(I \\pm D^{-1/2}YD^{-1/2})D^{1/2} \\\\\n",
    "&=D^{1/2}(I \\pm (D^{-1/2}Q_YD^{1/2})D^{-1}\\Lambda_Y(D^{-1/2}Q_YD^{1/2})^T)D^{1/2} \\quad\\quad \\text{(from claim 1)} \\\\\n",
    "&=D^{1/2}((D^{-1/2}Q_YD^{1/2})(D^{-1/2}Q_YD^{1/2})^T)D^{1/2} \\pm (D^{-1/2}Q_YD^{1/2})D^{-1}\\Lambda_Y(D^{-1/2}Q_YD^{1/2})^T)D^{1/2} \\\\\n",
    "&=Q_Y(D+\\Lambda_Y)Q_Y^T\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
