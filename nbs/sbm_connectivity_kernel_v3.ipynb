{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wish for a well-connected society? Have no more than $2\\sqrt{|\\mathrm{friends}|-1}$ number of friends who are similar to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stochastic Block Model\n",
    "\n",
    "Previously, we saw how in the case of a 1-dimensional categorical Blau space, the logistic connectivity kernel can be better represented as a stochastic block model (SBM), wherein nodes are members of a certain community $i \\in \\{1,2\\dots k\\}$ in the given Blau dimension, whose members form friendships according to a symmetric community-level probability matrix $\\Psi=\\{\\psi_{ij}\\}_{i,j=1}^k$.\n",
    "\n",
    "Let us consider a 1-dimensional categorical Blau space with $n$ people in $k$ communities; $k<n$. For each person $x_i \\in \\mathcal{X}$, we can write a corresponding membership vector $z_i \\in \\{0,1\\}^k$ containing exactly one $1$. (We could potentially relax this to mixed-membership vectors $z_i \\in [0,1]^k$ that sum to $1$.) Correspondingly, consider the assignment matrix $Z \\in \\{0,1\\}^{n\\times k}$. Then the stoachastic block model essentially suggests.\n",
    "\n",
    "$$A \\sim\\mathrm{Bernoulli}(Z\\Psi Z^T)$$\n",
    "\n",
    "Note that this can lead to self-loops, which we would ideally not like to have in the graph. Let us write instead for the \"simple\" graph $\\hat{A}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbb{E}[A|Z,\\Psi] &= Z\\Psi Z^T \\\\\n",
    "\\mathbb{E}[\\hat{A}|A] &= \\mathbb{E}[A|Z,\\Psi] - \\mathrm{diag}(\\mathbb{E}[A|Z,\\Psi])\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "1. Clearly, the $\\mathrm{rank}(A) \\le \\mathrm{min}(\\mathrm{rank}(Z), \\mathrm{rank}(\\Psi))$, which implies $\\mathrm{rank}(A) \\le k$. This satisfies our intuition that the adjacency matrix of an SBM model probably has a basis of the order of number of communities, and not number of people. This means that we can effectively reduce problems concerning with the adjacency matrix down to the level of the probability matrix instead, as we attempt to do below.\n",
    "2. Since this is a probabilistic model, we will mostly talk of the *expectation* of the adjacency matrix below.\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "Say we have somehow estimated the SBM matrix $\\Psi$, and are interested in the eigenspectrum of the corresponding (expected) adjacency matrix (more importantly perhaps the Laplacian matrix since it captures diffusion on the network). The above note suggests we should be able to do this at the level of $\\Psi$. For notational convenience, we'll indicate $\\mathbb{E}[A|Z,\\Psi]$ simply as $A = Z\\Psi Z^T$. (In essence, this treats the network as a weighted complete graph.)\n",
    "\n",
    "First, note that we can decompose $Z$ using singular value decomposition (SVD) as:\n",
    "\n",
    "\\begin{equation}\n",
    "Z = U\\Sigma V^T\n",
    "\\end{equation}\n",
    "\n",
    "Where $U$ is an orthogonal $n\\times n$ matrix whose columns are the left eigenvectors of $Z$, $V$ is an orthogonal $k\\times k$ matrix whose columns are the right eigenvectors of $Z$, and $\\Sigma$ is a $n\\times k$ retangular diagonal matrix that represents the singular values of $Z$. (Since Z is real, these are all real.) Let us further define two matrices: the (symmetric) community-count $k\\times k$ matrix $\\Pi = Z^TZ$, the (symmetric) co-membership $n\\times n$ matrix $M=ZZ^T$. For these two, we can write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi &= Z^TZ = V\\Sigma^T U^TU\\Sigma V^T = V\\Sigma^T\\Sigma V^T = V\\Lambda_\\Pi V^T\\\\\n",
    "M &= ZZ^T = U\\Sigma V^TV\\Sigma^T U^T = U\\Sigma\\Sigma^TU^T = U\\Lambda_M U^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\Lambda_\\Pi$ and $\\Lambda_M$ are diagonal matrices representing the eigenvalues of their respective matrices, whose diagonal elements are nothing but square of the singular values of $Z$.\n",
    "\n",
    "### Adjacency Matrix\n",
    "\n",
    "For simplicity, consider the \"uncorrected\" expected adjacency matrix $A$. \n",
    "\\begin{equation}\n",
    "A=Z\\Psi Z^T\n",
    "\\end{equation}\n",
    "Now, corresponding to eigenvectors of $A$, we have the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "A v &= \\lambda v \\\\\n",
    "Z \\Psi Z^T v &= \\lambda v \\\\\n",
    "(Z^TZ)\\Psi Z^T v &= \\lambda Z^T v \\\\\n",
    "(\\Pi\\Psi)(Z^Tv) &= \\lambda (Z^T v) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, the eigenvalues of $A$ correspond to eigenvalues of the product $\\Pi\\Psi$, and the eigenvectors of $\\Pi\\Psi$ correspond to $Z^T$ times the eigenvectors of $A$. More precisely, we can write the eigenvector $v$ of $A$ in terms of eigenvector $u$ of $\\Pi\\Psi$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Z^Tv &= u \\\\\n",
    "v &= (Z^T)^+u \\quad\\quad&\\text{where }X^+\\text{ refers to the Moore-Penrose pseudoinverse of }X \\\\\n",
    "v &= (V\\Sigma^TU^T)^+u\\\\\n",
    "v &= U(\\Sigma^T)^+V^Tu \\\\\n",
    "v &= U(\\Sigma^+)^TV^Tu \\quad\\quad&\\text{where }\\Sigma^+ \\text{ is simply replacing its non-zero diagonal elements by their inverse, followed by a transpose}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus we can easily solve for the spectrum of $A$ by solving for the spectrum of $\\Pi\\Psi$. A key thing to note here is that while $\\Psi$ decides the inter-community probability matrix, $\\Pi$ is a measure of the composition of our \"test\" population. It appears sensible then that the spectrum of $A$ should depend not only on how much communities discriminate against one another, but how much of each community exists in the society as well.\n",
    "\n",
    "We note that if the inverse of $\\Pi=Z^T Z$ exists (if community membership columns are linearly independent, which is likely the case, in particular for single-memberships wherein every community as at least one member) then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Z^+ &= \\Pi^{-1}Z^T \\\\\n",
    "&= V\\Lambda_\\Pi^{-1}V^T(U\\Sigma V^T)^T \\\\\n",
    "&= V\\Lambda_\\Pi^{-1}\\Sigma^TU^T \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Comparing this to what we would obtain by SVD of $Z$, we get that $\\Sigma^+ = \\Lambda_\\Pi^{-1}\\Sigma^T$. Therefore, $Z^+Z=Z^T(Z^T)^+=I_k$. Moreover, very simply, eigenvectors of $A$ are given by $v = Z\\Pi^{-1}u$.\n",
    "\n",
    "### Laplacian Matrix\n",
    "\n",
    "For studying dynamics on a network, we would be interested in other matrices which are some kind of a derivative of the original Adjacency matrix. Given the analysis above, the spectrum of such matrices should also be deducible from the spectrum of some rank $k$ matrix.\n",
    "\n",
    "Let us consider the expected Laplacian matrix which governs diffusion processes on a graph, given by $L=D-\\hat{A}$, where $D=\\mathrm{diag}(\\hat{A}\\boldsymbol j)$ is a $n\\times n$ diagonal matrix capturing the expected degree of a node, $\\hat{A}$ is the expected adjacency matrix, $\\boldsymbol j = \\{1\\}^n$ is the unit vector. Because of how $L$ is defined, any self-loops in the graph are \"cancelled\" out, and indeed we can directly write $L$ in terms of $A$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L &= \\mathrm{diag}(A\\boldsymbol j) - A \\\\\n",
    "L &= Z(\\Pi^{-1}Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z\\Pi^{-1} - \\Psi)Z^T \\\\\n",
    "L &= Z(Z^+\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)(Z^+)^T - \\Psi)Z^T \\\\\n",
    "L &= Z\\mathcal{L}Z^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Therefore, we can represent the Laplacian $L$ in terms of a rank-reduced representation $\\mathcal{L}=Z^+\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)(Z^+)^T -\\Psi$, analogous to how we represented $A$ in terms of $\\Psi$. The eigenvalues and eigenvectors of $L$ thus analogously follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Eigenvalue Inequalities\n",
    "\n",
    "As noted above, we did not remove \"self-loops\" while estimating eigenvalues of the expected adjacency matrix. Ideally, we would like to do this, which involves subtracting the diagonal of the uncorrected adjacency matrix from itself. Moreover, to evaluate eigenvalues at the network level we need the eigenvalues of product of two matrices at the community-level. Although we can numerically perform both these operations of addition and multiplication, we might nevertheless be interested in closed-form estimates of the eigenvalues. For that purpose, we refer to Horn's inequalities that compare the eigenvalues of matrix sums or matrix products in terms of the eigenvalues of its constituents. (Although these results hold in general for any matrices, we are particularly interested in what happens when we add a diagonal matrix (to correct for the self-loops) or multiply a diagonal matrix (since under a weak assumption of single-memberships, $\\Pi$ is simply a positive diagonal matrix, and calculation of other varians of the graph Laplacian also involve some products using the diagonal degree matrix.)\n",
    "\n",
    "#### For Addition\n",
    "\n",
    "For real symmetric matrices $A, B, C=A+B$ with eigenvalues $\\{\\alpha_1 \\ge \\alpha_2 \\dots \\ge \\alpha_n\\},\\{\\beta_1 \\ge \\beta_2 \\dots \\ge \\beta_n\\},\\{\\gamma_1 \\ge \\gamma_2 \\dots \\ge \\gamma_n\\}$, we have the following [Weyl's inequalities](https://en.wikipedia.org/wiki/Weyl's_inequality):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{max}(\\{\\alpha_i+\\beta_j; i+j \\ge k+n\\}) \\le \\gamma_k \\le \\mathrm{min}(\\{\\alpha_i+\\beta_j; i+j \\le k+1\\})\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, from the property of sum of eigenvalues being equal to trace of the matrix, we have the following equality:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_i \\gamma_i = \\sum_i \\alpha_i + \\sum_i\\beta_i\n",
    "\\end{equation}\n",
    "\n",
    "#### For Multiplication\n",
    "\n",
    "For real symmetric matrices $A, B, C=AB$ with eigenvalues $\\{\\alpha_1 \\ge \\alpha_2 \\dots \\ge \\alpha_n\\},\\{\\beta_1 \\ge \\beta_2 \\dots \\ge \\beta_n\\},\\{\\gamma_1 \\ge \\gamma_2 \\dots \\ge \\gamma_n\\}$, they follow [exponentiated version of Weyl's inequalities](https://math.stackexchange.com/questions/20302/eigenvalues-of-product-of-a-matrix-and-a-diagonal-matrix)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{max}(\\{\\alpha_i\\beta_j; i+j \\ge k+n\\}) \\le \\gamma_k \\le \\mathrm{min}(\\{\\alpha_i\\beta_j; i+j \\le k+1\\})\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, from the property of product of eigenvalues being equal to determinant of the matrix, we have the following equality:\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_i \\gamma_i = \\prod_i \\alpha_i \\prod_i\\beta_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity Kernel as an SBM\n",
    "\n",
    "Let's recapitulate the design of the SBM matrix $\\Psi$ for the kind of social survey data we might have. We enforce a weak assumption of single-membership, that is a person can only belong to one community in the given categorical Blau dimension. Let $\\boldsymbol\\rho\\in[0,1]^k$ be a vector of \"proportion of friends\" parameters for the $k$ communities, that is $\\rho_i$ represents for the $i$th community the \"average\" proportion of their friends that are from within the community. Let $\\boldsymbol\\pi\\in(0,1)^k$ be the \"proportion of population\" parameters for the $k$ communities, that is $\\pi_i$ represents what proportion of the \"training\" population are people from community $i$ and thus $\\sum_i\\pi_i=1$. Let $\\omega$ be the mean edge density parameter. Then under the reasonable assumption that a \"source\" community discriminates against members of any other \"target\" community regardless of what that target community is (**\"equally discriminatory\" assumption**), we can succinctly write the symmetric SBM block matrix as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Psi_{ii} &= \\omega\\frac{\\rho_i}{\\pi_i} \\\\\n",
    "\\Psi_{ij} = \\Psi_{ji} &= \\frac{\\omega}{2}\\left(\\frac{1-\\rho_i}{1-\\pi_i} + \\frac{1-\\rho_j}{1-\\pi_j}\\right) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "Let $\\boldsymbol{\\hat{\\pi}}\\in[0,1]^k$ represent the distribution of communities in a \"testing\" population with assignments $Z$, that is $\\sum_i\\hat{\\pi_i}=1$. Clearly, $\\Pi=Z^TZ = n*\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}})$. As described above, the eigenvalues of the \"uncorrected\" expected adjacency matrix is given by the eigenvalues of\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi\\Psi = n*\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}})\\Psi\n",
    "\\end{equation}\n",
    "\n",
    "Let us explicitly write the values in this matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\Pi\\Psi)_{ii} &= n\\Psi_{ii}\\hat{\\pi_i} = \\omega n\\rho_i\\frac{\\hat{\\pi_i}}{\\pi_i} \\\\\n",
    "(\\Pi\\Psi)_{ij} &= n\\Psi_{ij}\\hat{\\pi_i} = \\frac{\\omega n}{2}\\left((1-\\rho_i)\\frac{\\hat{\\pi_i}}{1-\\pi_i} + (1-\\rho_j)\\frac{\\hat{\\pi_i}}{1-\\pi_j}\\right) \\\\\n",
    "(\\Pi\\Psi)_{ji} &= n\\Psi_{ji}\\hat{\\pi_j} = \\frac{\\omega n}{2}\\left((1-\\rho_j)\\frac{\\hat{\\pi_j}}{1-\\pi_j} + (1-\\rho_i)\\frac{\\hat{\\pi_j}}{1-\\pi_i}\\right) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Correspondingly for the Laplacian $L$, we need to estimate eigenvalues of $\\mathcal{L} = \\Pi^{-1}Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z\\Pi^{-1} - \\Psi$. We have the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j))_{xx} &= n\\sum_j\\Psi_{ij}\\hat{\\pi_j} \\quad\\quad\\text{where } Z_{xi}=1\\\\\n",
    "(Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z)_{ii}  &= n^2\\hat{\\pi_i}\\sum_j\\Psi_{ij}\\hat{\\pi_j} \\\\\n",
    "(\\Pi^{-1}Z^T\\mathrm{diag}(Z\\Psi Z^T\\boldsymbol j)Z\\Pi^{-1})_{ii}  &= \\sum_j\\Psi_{ij}\\frac{\\hat{\\pi_j}}{\\hat{\\pi_i}} \\\\\n",
    "\\mathcal{L}_{ii} &= \\sum_{j\\neq i}\\Psi_{ij}\\frac{\\hat{\\pi_j}}{\\hat{\\pi_i}} \\\\\n",
    "\\mathcal{L}_{ij} = \\mathcal{L}_{ji} &= -\\Psi_{ij} = -\\Psi_{ji} \\\\\n",
    "\\mathcal{L} &= \\mathrm{diag}(\\Pi^{-1}\\Psi\\Pi\\boldsymbol{j}) -\\Psi\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus for $L$ we can write the eigenvalues of $\\Pi\\mathcal{L} = n*\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}})\\mathcal{L}$. Let us succinctly write the values of this matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\Pi\\mathcal{L})_{ii} &= n\\sum_{j\\neq i}\\Psi_{ij}\\hat{\\pi_j} \\\\\n",
    "(\\Pi\\mathcal{L})_{ij} &= -n\\Psi_{ij}\\hat{\\pi_i} \\\\\n",
    "(\\Pi\\mathcal{L})_{ji} &= -n\\Psi_{ji}\\hat{\\pi_j} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus the eigenvalues of $L$ are given by the eigenvalues of:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi\\mathcal{L} = \\mathrm{diag}(\\Psi\\Pi\\boldsymbol{j})-\\Pi\\Psi\n",
    "\\end{equation}\n",
    "\n",
    "### For Multiple Blau Dimensions\n",
    "\n",
    "Let us consider $m$ independent categorical Blau dimensions, each with $\\{k_1,k_2,\\dots k_m\\}$ number of communities; $\\prod_{y=1}^mk_y < n$. (The consideration of independence must be appropriate, for instance one might like to consider \"**income:** low, middle, high\" and \"**job:** unemployed, employed\" as a single \"**income X job**:\" categorical with $3\\times 2=6$ communities.) Correspondingly, we have their respective parameters $\\{\\boldsymbol\\rho_1\\in[0,1]^{k_1},\\boldsymbol\\rho_2\\in[0,1]^{k_2},\\dots \\boldsymbol\\rho_m\\in[0,1]^{k_m}\\}$, $\\{\\boldsymbol\\pi_1\\in(0,1)^{k_1},\\boldsymbol\\pi_2\\in(0,1)^{k_2},\\dots \\boldsymbol\\pi_m\\in(0,1)^{k_m}\\}$, and \"testing\" population proportions $\\{\\boldsymbol{\\hat{\\pi_1}}\\in[0,1]^{k_1},\\boldsymbol{\\hat{\\pi_2}}\\in[0,1]^{k_2},\\dots \\boldsymbol{\\hat{\\pi_m}}\\in[0,1]^{k_m}\\}$. Under the independence assumption, we can simply appropriately multiply dimension-specific probabilities from $\\Psi_y$ to obtain overall block matrix $\\Psi$.\n",
    "\n",
    "Let us define dimension-specific unsymmetric block matrices $\\overrightarrow{\\Psi_y}$ such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\overrightarrow{\\Psi_y})_{ii} &= \\frac{\\rho_{yi}}{\\pi_{yi}} \\\\\n",
    "(\\overrightarrow{\\Psi_y})_{ij} &= \\frac{1-\\rho_{yi}}{1-\\pi_{yi}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "For the overall SBM, we can define the $\\prod_yk_y\\times\\prod_yk_y$ sized block matrix as a Kroenecker product of dimension-specific block matrices.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\overrightarrow{\\Psi_\\otimes} &= \\omega \\bigotimes_{y=1}^m\\overrightarrow{\\Psi_y} \\\\\n",
    "\\Psi_{+\\otimes} &= \\frac{\\overrightarrow{\\Psi_\\otimes}+\\overrightarrow{\\Psi_\\otimes}^T}{2}\\\\\n",
    "\\Psi_{+\\otimes} &= \\frac{\\omega}{2}\\left(\\bigotimes_{y=1}^m\\overrightarrow{\\Psi_y} + \\bigotimes_{y=1}^m\\overrightarrow{\\Psi_y}^T\\right) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Note that what we obtain is not necessarily the same as what we'd have got on \"symmetrisizing\" before combining the dimensions as in $\\Psi_{\\otimes +} = \\omega\\bigotimes_{y=1}^m\\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2}$. The former is a kind of sum-of-products while the latter a product-of-sums. We can explicitly write values of these matrices as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Psi_{+\\otimes}[(r_1\\times r_2\\times \\dots r_m), (c_1\\times c_2\\times \\dots c_m)] &= \\frac{\\omega}{2}\\left(\\prod_{y=1}^m(\\overrightarrow{\\Psi_y})_{r_yc_y}+\\prod_{y=1}^m(\\overrightarrow{\\Psi_y})_{c_yr_y}\\right)\\\\\n",
    "\\Psi_{\\otimes +}[(r_1\\times r_2\\times \\dots r_m), (c_1\\times c_2\\times \\dots c_m)] &= \\omega\\prod_{y=1}^m\\frac{(\\overrightarrow{\\Psi_y})_{r_yc_y}+(\\overrightarrow{\\Psi_y})_{c_yr_y}}{2}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "One might argue that $\\Psi_{+\\otimes}$ is more appropriate, since it symmetrisizes only once, but $\\Psi_{\\otimes +}$ is likely to be more \"stable\". Let us pick this as $\\Psi=\\Psi_{\\otimes +}$.\n",
    "\n",
    "Along similar lines, we can define $\\boldsymbol{\\hat{\\pi}} = \\mathrm{vec}(\\dots\\mathrm{vec}(\\mathrm{vec}(\\boldsymbol{\\hat{\\pi}}_1\\boldsymbol{\\hat{\\pi}}_2^T)\\boldsymbol{\\hat{\\pi}}_3^T)\\dots\\boldsymbol{\\hat{\\pi}}_m^T)$. This leads to the definition of $\\Pi$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pi = n\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y)\n",
    "\\end{equation}\n",
    "\n",
    "As before, $\\Psi$ and $\\Pi$ can be straightforwardly plugged into the formulae described in the previous section, for eigenvalues of $A$ and $L$. With a bit of algebraic manipulation, we can write them out quite succinctly. First, we use the mixed-product property of the Kroenecker product, i.e. $(A_{n\\times n}\\otimes B_{m\\times m})(C_{n\\times n}\\otimes D_{m\\times m}) = (AC\\otimes BD)$. Second, we note that $\\mathrm{diag}((A_{n\\times n}\\otimes B_{m\\times m})\\boldsymbol{j}_{nm}) = \\mathrm{diag}(A\\boldsymbol{j}_m) \\otimes \\mathrm{diag}(B\\boldsymbol{j}_n)$. Using these two, we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi\\Psi &= n\\omega\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\\\\n",
    "\\Pi\\mathcal{L} &= \\mathrm{diag}(\\Psi\\Pi\\boldsymbol{j}_{k_1\\times\\dots k_m}) - \\Pi\\Psi \\\\\n",
    "\\Pi\\mathcal{L} &= n\\omega\\left\\{\\bigotimes_{y=1}^m \\mathrm{diag}\\left(\\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\boldsymbol{j}_{k_y}\\right) - \\bigotimes_{y=1}^m\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2}\\right\\}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that the rank of these matrices is now $\\prod_{y=1}^mk_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Connectivity Kernel as an SBM\n",
    "\n",
    "Previously, we had assumed that a \"source\" community discriminates against members of any other \"target\" community regardless of what that target community is. Let us add another assumption to this one, that it is regardless of what the source community is either. That is, every community favours members of its own just as much as every other community, and every community discriminates against members of another community just as much as every other community (**\"identically equally discriminatory\"  (IED) assumption**). This effectively assumes that the training population is equally distributed amongst all communities, and reduces the number of learnable parameters of a single-dimension categorical Blau dimension model to just one $\\rho$ -- the \"forward\" kernel interpretation of Till's logistic connectivity kernel.\n",
    "\n",
    "Let $\\boldsymbol\\rho\\in[0,1]^m$ be a vector of \"proportion of friends\" parameters for the $m$ Blau dimensions, that is $\\rho_y$ represents for the $y$th Blau dimension the \"average\" proportion of friends that are from within a community. Let $\\boldsymbol\\pi\\in(0,1)^m$ be the \"proportion of population\" parameter, that is $n\\pi_y$ represents the average community size in the \"training\" population for the $y$th Blau dimension. Alternatively, we can have $\\boldsymbol k\\in\\{2,3,\\dots\\}^m$ represent the number of communities, and therefore $\\pi_y = \\frac{1}{k_y}$ (which are usually known in the survey design). This is a more intuitive parametrization under the IED assumption, so we use $\\boldsymbol k$ now instead of $\\boldsymbol\\pi$. Let $\\omega$ be the mean edge density parameter. We have the following for $\\Psi$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\Psi_y)_{ii} &= k_y\\rho_y \\\\\n",
    "(\\Psi_y)_{ij} = (\\Psi_y)_{ji} &= \\frac{1-\\rho_y}{1-1/k_y} \\\\\n",
    "\\Psi_{+\\otimes} = \\Psi_{\\otimes +} = \\Psi &= \\omega \\bigotimes_{y=1}^m\\Psi_y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "As before, for the expectation of \"uncorrected\" adjacency matrix (and expectation of Laplacian matrix), its eigenvalues are same as the eigenvalues of $\\Pi\\Psi$ (and of $\\Pi\\mathcal{L}$), which we can write as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi\\Psi &= n\\omega\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\Psi_y \\\\\n",
    "\\Pi\\mathcal{L} &= n\\omega\\left\\{ \\bigotimes_{y=1}^m \\mathrm{diag}(\\Psi_y \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\boldsymbol{j}_{k_y}) - \\bigotimes_{y=1}^m\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\Psi_y\\right\\}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We can solve for the above numerically. To obtain a closed form solution, we can use Weyl's inequalities. But to obtain exact closed-form solutions, we make another assumption that the **\"testing\" and \"training\" population is identically distributed**. (This assumption should be mostly valid, unless we wish to hypothesize about novel social setups wherein given $\\Psi$, the population proportions $\\Pi$ are tweaked to achieve certain \"desirable\" properties.) Consequently, since the training population is assumed to be equi-proportioned in IED, this makes the testing population also equiproportioned. This would imply that $(\\boldsymbol{\\hat{\\pi_y}})_i = \\pi_y = \\frac{1}{k_y}$. Therefore, we have the following simple relationships:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "(\\widetilde{\\Psi}_y)_{ii} &=  \\rho_y \\\\\n",
    "(\\widetilde{\\Psi}_y)_{ij} = (\\widetilde{\\Psi}_y)_{ji} &=  \\frac{1-\\rho_y}{k_y-1} \\\\\n",
    "\\Pi\\Psi &= n\\omega \\bigotimes_{y=1}\\widetilde{\\Psi}_y \\\\\n",
    "\\Pi\\mathcal{L} &= n\\omega \\left(\\mathrm{I} - \\bigotimes_{y=1}\\widetilde{\\Psi}_y\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Now for the Kroenecker product, we use the following fact. Suppose $A_{n\\times n}$ and $B_{m\\times m}$ are square matrices with eigenvalues $\\alpha_1, \\dots \\alpha_n$ and $\\beta_1, \\dots \\beta_m$ listed by multiplicity, and eigenvectors $u_1,\\dots u_n$, $v_1,\\dots v_m$. Then the eigenvalues of $A\\otimes B$ are given by $\\alpha_i\\beta_j$, and corresponding eigenvectors by $\\mathrm{vec}(u_iv_j^T)$. That is, we simply need to find eigenvalues/eigenvectors separately for all $\\widetilde{\\Psi}_y$ and perform their outer-product in order to compute eigenvalues/eigenvectors of $A$ and $L$.\n",
    "\n",
    "Let $Y$ be a real symmetric matrix, and $\\alpha$, $\\beta$ be scalars. Thus $Y$ can be eigendecomposed into $Q_Y\\Lambda_Y Q_Y^T$ such that $Q_Y$ is an orthonormal matrix ($Q_YQ_Y^T=Q_Y^TQ_Y=I$) whose columns are the eigenvectors of $Y$, and $\\Lambda_Y$ is a diagonal matrix containing real eigenvalues of $Y$.\n",
    "\n",
    "#### Claim: For symmetric matrix $X=\\alpha\\mathrm{I}+\\beta Y$ we have eigenvalues $\\Lambda_X=\\alpha\\mathrm{I}+\\beta\\Lambda_Y$ and eigenvectors $Q_Y$\n",
    "\n",
    "Proof:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X &= \\alpha\\mathrm{I}+\\beta Y \\\\\n",
    "&= \\alpha Q_YQ_Y^T+\\beta Q_Y\\Lambda_YQ_Y^T \\\\\n",
    "&= Q_Y(\\alpha\\mathrm{I} + \\beta\\Lambda_Y)Q_Y^T \\\\\n",
    "&= Q_X\\Lambda_X Q_X^T\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, $\\Lambda_X = \\alpha\\mathrm{I} + \\beta\\Lambda_Y$ is a diagonal matrix, and $Q_X=Q_Y$ is orthogonal.\n",
    "\n",
    "We can use the above claim to easily find eigenvalues of $\\widetilde{\\Psi}_y$. First, we write them as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\widetilde{\\Psi}_y &= \\frac{k_y\\rho_y-1}{k_y-1}\\mathrm{I} + \\frac{1-\\rho_y}{k_y-1}\\mathcal{I}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Now we know eigenvalues of the identity matrix $\\mathrm{I}_n$ are $1$ with multiplicity $n$, and of the all-ones matrix $\\mathcal{I}_n$ are $n$ with multiplicity $1$ and $0$ with multiplicity $n-1$. Thus we have the following:\n",
    "\n",
    "1. Eigenvalues of $\\widetilde{\\Psi}_y$ are $1$ with multiplicity $1$ and $\\large\\frac{k_y\\rho_y-1}{k_y-1}$ with multiplicity $k_y-1$.\n",
    "\n",
    "2. Corresponding (normalized) eigenvectors are given by those of the all-ones matrix:\n",
    "    1. For eigenvalue $\\lambda^y_0=1$: $\\boldsymbol{v}^y_0 = \\left\\{\\frac{1}{\\sqrt{k_y}}, \\frac{1}{\\sqrt{k_y}} \\dots \\frac{1}{\\sqrt{k_y}}\\right\\}$\n",
    "    2. For eigenvalue $\\lambda^y_i=\\frac{k_y\\rho_y-1}{k_y-1}$ (where $i\\in\\{1,2,\\dots k_y-1\\}$): the nullspace of $\\mathcal{I}$, which implies for eigenvector $\\boldsymbol{v}^y_i = \\{v^y_{i1},v^y_{i2}\\dots v^y_{ik_y}\\}$ that $\\sum_{j=1}^{k_y}v^y_{ij}=0$. Let us choose these $k_y-1$ eigenvectors to be orthonormal to each other and to the first eigenvector $\\boldsymbol{v}^y_0$ using the standard orthonormal basis of $\\mathbb{R}^{k_y-1}$ followed by the [Gram-Schmidt Process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) as: $\\boldsymbol{v}^y_i =\\left\\{-\\frac{1}{\\sqrt{i(i+1)}}\\right\\}^i ; \\left\\{\\sqrt{\\frac{i}{i+1}}\\right\\} ; \\{0\\}^{k_y-i-1}$, where $;$ refers to an augmentation.\n",
    "\n",
    "3. Eigenvalues of $A$ are:\n",
    "    1. $n\\omega$ with multiplicity $1$\n",
    "    2. $n\\omega \\large\\frac{k_i\\rho_i-1}{k_i-1}$ with multiplicity $(k_i-1)$ evaluated over every $i\\in\\{1,2,\\dots m\\}$ Blau dimensions\n",
    "    3. $n\\omega \\large\\frac{k_i\\rho_i-1}{k_i-1}\\frac{k_j\\rho_j-1}{k_j-1}$ with multiplicity $(k_i-1)(k_j-1)$ evaluated over each $i,j\\in\\{1,2,\\dots m\\}; i\\neq j$ pair of Blau dimensions\n",
    "    4. $\\dots$\n",
    "    5. $n\\omega \\large\\prod_{y=1}^m\\frac{k_y\\rho_y-1}{k_y-1}$ with multiplicity $\\prod_{y=1}^m(k_y-1)$ evaluated over all Blau dimensions\n",
    "\n",
    "4. It is straightforward to note a structure to the above set of eigenvalues for $A$, wherein we are essentially making \"product-combinations\" of eigenvalues of $\\widetilde{\\Psi}_y$s. Each of those matrices have two possible eigenvalues, giving us a sequence of $m$ independent Bernoulli \"coin-tosses\" ($1$ can be treated as $H$ (heads) and $\\frac{k_i\\rho_i-1}{k_i-1}$ as $T$ (tails)) of parameters $\\{1/k_y\\}_{y=1}^m$. If we had $k_1=k_2=\\dots=k_m=k$, then this is a Binomial distribution with parameters $m$ and $1/k$. Of course in the general SBM setting, each $\\widetilde{\\Psi}_y$ will have $k_y$ number of eigenvalues, giving as a sequence of $m$ independent Categorical \"die-throws\" where the $i$th die is $k_i$-sided. Again, if we had $k_1=k_2=\\dots=k_m=k$, then this is a Multinomial distribution with parameters $m$ and $\\{1/k\\}^k$. **This means that in general, the distribution of eigenvalues should resemble something close to a multinomial distribution.** Recall that these is a distribution over \"discrete\" eigenvalue combinations, but given that we can order these eigenvalue combinations by their value, this should essentially capture the distribution of eigenvalues of $A$.\n",
    "\n",
    "5. Eigenvectors of $A$ are simply $u = Z\\Pi^{-1}v = (\\prod_{y=1}^mk_y/n)Zv$, where $v$ are given by $\\mathrm{vec}(\\bigotimes_{y=1}^m \\boldsymbol{v}^y_{j_y})$ for all combinations of $j_y\\in \\{0,1,\\dots k_y\\}$. Enumerating all eigenvectors is a redundant exercise, but (a) assuming that the adjacency matrix is arranged according to the most homophilous dimension $\\hat{y}$, and (b) assuming WLOG that $\\hat{y}=m$, and (c) choosing the last eigenvector of $\\widetilde{\\Psi}_\\hat{y}$ to represent the nullspace, we write the first two eigenvectors:\n",
    "    \n",
    "    1. $u_0 = \\large\\left\\{\\frac{\\sqrt{\\prod_{y=1}^mk_y}}{n}\\right\\}^n$\n",
    "    2. $u_1 = \\large\\left\\{\\frac{\\sqrt{\\prod_{y=1}^mk_y}\\sqrt{k_\\hat{y}-1}}{n}\\right\\}^{n/k_\\hat{y}} ; \\left\\{-\\frac{\\sqrt{\\prod_{y=1}^mk_y}}{n\\sqrt{k_\\hat{y}-1}}\\right\\}^{n-n/k_\\hat{y}}$\n",
    "    \n",
    "6. Eigenvalues of $L$ are simply $n\\omega$ minus the eigenvalues of $A$, that is:\n",
    "    1. $0$ with multiplicity $1$\n",
    "    2. $n\\omega \\large\\frac{k_i(1-\\rho_i)}{k_i-1}$ with multiplicity $(k_i-1)$ evaluated over every $i\\in\\{1,2,\\dots m\\}$ Blau dimensions\n",
    "    3. $n\\omega \\large\\frac{k_ik_j(1-\\rho_i\\rho_j) - k_i(1-\\rho_i) - k_j(1-\\rho_j)}{(k_i-1)(k_j-1)}$ with multiplicity $(k_i-1)(k_j-1)$ evaluated over each $i,j\\in\\{1,2,\\dots m\\}; i\\neq j$ pair of Blau dimensions\n",
    "    4. $\\dots$\n",
    "    5. $n\\omega \\large\\frac{\\sum_{i=1}^m (-1)^{m-i} \\left\\{\\sum_{1\\le y_1\\le y_2 \\dots y_i\\le m}\\left(\\prod_{j=1}^ik_{y_j}\\right)\\left(1-\\prod_{j=1}^i\\rho_{y_j}\\right)\\right\\}}{\\prod_{y=1}^m(k_y-1)}$ with multiplicity $\\prod_{y=1}^m(k_y-1)$ evaluated over all Blau dimensions\n",
    "\n",
    "#### Eigenvalues of $A$\n",
    "1. Note that for all $y$, since $0 \\le \\rho_y \\le 1$ and $k_y\\ge 2$, $-1 \\le\\frac{k_y\\rho_y-1}{k_y-1} \\le 1$. That is, the eigenvalues for $A$ are listed in terms of \"almost\" monotonically non-increasing absolute values, varying between $-n\\omega$ and $n\\omega$.\n",
    "2. The largest eigenvalue $\\lambda_1$ of $A$ is known to correspond to average degree of the graph. Clearly, $\\lambda_1=n\\omega$ is the mean degree of every node in an $n$ sized network. Moreover, for d-regular graphs, all eigenvalues lie between $[-d,d]$, which is also what we note here. (See [this](http://www.cs.yale.edu/homes/spielman/561/2012/lect03-12.pdf).)\n",
    "3. The smallest eigenvalue of $A$ equals $-d$ if and only if the graph is bipartite. This is easy to see if we let $m=1$, and for that $\\rho=0$ and $k=2$. Indeed, a perfectly heterophilous kernel renders a complete bipartite graph. Similarly, for any other value of $k>2$ we would obtain a complete k-partite graph $K_{n/k,n/k,\\dots n/k}$.\n",
    "4. Recall (from the previous notebook) that the $y$th Blau dimension is heterophilous if $k_y\\rho_y <1$, ambiphilous if $k_y\\rho_y =1$ and homophilous if $k_y\\rho_y >1$. In this light, a homophilous (heterophilous) Blau dimension contributes towards a positive (negative) eigenvalue, and ambiphilous to $0$ eigenvalues. More elaborately, we have:\n",
    "    1. If and only if all Blau dimensions are homophilous then all eigenvalues of $A$ are positive.\n",
    "    2. If one or more Blau dimensions are heterophilous then at least some eigenvalues are negative. \n",
    "    3. If one or more Blau dimensions are ambiphilous then at least some eigenvalues are 0.\n",
    "    4. In particular, all eigenvalues (other than $\\lambda_0$) are non-positive if and only if no more than one dimension is heterophilous and the rest are ambiphilous.\n",
    "\n",
    "#### Eigenvalues of $L$\n",
    "1. Note that for all $y$, since $0 \\le \\rho_y \\le 1$ and $k_y\\ge 2$, all eigenvalues of $L$ lie between $0$ and $2n\\omega$.\n",
    "2. Since $L$ is a positive-semidefinite matrix (it is diagonally dominant), it has non-negative eigenvalues, which is also what we note here. The smallest eigenvalue of $L$ is $0$.\n",
    "3. The number of \"connected\" components correspond to the algebraic multiplicity of $0$, which is $1$ if none of the Blau dimensions are perfectly homophilous ($\\rho_y\\neq 1$). If one of them, say $y=1$ is perfectly homophilous, we immediately obtain $1+(k_1-1)=k_1$ connected components. If two of them, say additionally $y=2$ is perfectly homophilous, we obtain $1+(k_1-1)+(k_2-1)+(k_1-1)(k_2-1) = k_1k_2$ connected components, and so on. This is perfectly in sync with our intuition of the stochastic block model.\n",
    "5. The second-smallest eigenvalue of $L$ corresponds to the algebraic connectivity of the graph. Let us pick the most \"anti-ambiphilous\" Blau dimension that corresponds to this eigenvalue, then on dropping the subscripts this value is given by $n\\omega \\large\\frac{1-\\rho}{1-1/k}$. Clearly, algebraic connectivity is:\n",
    "    1. $\\propto n$, the number of people\n",
    "    2. $\\propto \\omega$, the mean edge density\n",
    "    3. inversely proportional to $k$, the number of components\n",
    "    4. inversely proportional to $\\rho$, the proportion of friends of same community\n",
    "    5. less (more) than the average degree $n\\omega$ for a homophilous (heterophilous) kernel\n",
    "6. The eigenvector corresponding to the second-smallest eigenvalue of $L$, termed as the Fiedler vector, can be used to \"partition\" the nodes into clusters. This eigenvector corresponds here to the eigenvector of the second-largest eigenvalue of $A$: $u_1$ above. Evidently, the Fiedler vector assigns \"positive\" values to nodes of 1 of the $k_{\\hat{y}}$ communities, and negative to the rest. The other (unlisted) eigenvectors $u_2,u_3,\\dots u_{k_{\\hat{y}}-1}$ should correspondingly \"single-out\" the remaining $k_{\\hat{y}}-1$ communities according to this Blau dimension. These would then be followed by eigenvectors corresponding to the *next* most homophilous Blau dimension, possibly pairs of them, and so on, elucidating a hierarchical sense of clusters that mimics the intuition of the SBM: more (less) homophilous dimensions divide at the higher (lower) scales.\n",
    "7. The smallest non-zero eigenvalue of $L$ is called the spectral gap. This is essentially given by the most homophilous Blau dimension after ignoring all perfectly homophilous dimensions. (If there are no homophilous dimensions, then given by the pair of most heterophilous dimensions.) If the graph is connected, this should coincide with the second-smallest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Eigenvalues of $A$ for Connectedness\n",
    "\n",
    "The above closed-form solutions on the expectation of (uncorrected) $A$ can be further used to derive some interesting results about the social network.\n",
    "\n",
    "#### Cheeger's Inequalities for Expander Graphs\n",
    "\n",
    "Since the \"expected\" degree of the graph turns out to be the same for every node, this is equivalent to a d-regular graph where $d=\\omega n$. We can thus apply techniques from theory of expander graphs. Expander graphs are sparse graphs with strong connectivities, and thus could be relevant to robust social networks.\n",
    "\n",
    "Let $S$ be a subset of vertices from the graph $A$. Let $\\partial S$ represent the edge boundary of $S$, that is the number of edges that \"cross\" the boundary of this set. The isoparametric number or Cheeger constant $h(A)$ is defined as the minimum of $\\frac{|\\partial S|}{|S|}$ when evaluated over all such $S$ such that $|S|\\le n/2$. Clearly, this constant can be seen as some notion of a \"bottleneck\" in the graph. (The smaller it is, the more bottlenecked the network is.) For expander graphs, this constant must be high.\n",
    "\n",
    "Given the interpretation of $A$ as a regular graph, we can use Cheeger's inequaities which relates eigenvalues of $A$ ($\\lambda_0 \\ge \\lambda_1 \\ge \\lambda_2 \\dots \\ge \\lambda_{n-1}$)to bounds on $h(A)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\lambda_0-\\lambda_1) \\le h(A) \\le \\sqrt{2\\lambda_0(\\lambda_0-\\lambda_1)}\n",
    "\\end{equation}\n",
    "\n",
    "Now for our simplified connectivity kernel, the first eigenvalue of $A$, $\\lambda_0=n\\omega$, and assuming there exists at least one homophilous dimension, the second eigenvalue of $A$ is given by $\\lambda_1=n\\omega*\\mathrm{max}\\left\\{\\frac{k_y\\rho_y-1}{k_y-1}; \\forall y\\in\\{1,2,\\dots m\\}\\right\\}$.\n",
    "\n",
    "To find the maximum of the argument, we observe the following for two homophilous dimensions $i$ and $j$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\frac{k_i\\rho_i-1}{k_i-1} \\ge \\frac{k_j\\rho_j-1}{k_j-1} \\\\\n",
    "\\implies & k_ik_j(\\rho_i-\\rho_j) \\ge k_j(1-\\rho_j) + k_i(\\rho_i-1) \\\\\n",
    "\\implies & \\text{if } \\rho_i=\\rho_j \\text{ then }  k_i \\ge k_j \\\\\n",
    "\\implies & \\text{if } k_i=k_j \\text{ then }  \\rho_i \\ge \\rho_j \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, $i$ is the *most homophilous* dimension. When one of $i,j$ is homophilous and the other heterophilous, then the homophilous dimension certainly maximizes the argument more. Let us consider the (unlikely) case when all Blau dimensions are heterophilous, then $\\lambda_1=n\\omega*\\mathrm{max}\\left\\{\\frac{k_y\\rho_y-1}{k_y-1}\\frac{k_x\\rho_x-1}{k_x-1}; \\forall y,x\\in\\{1,2,\\dots m\\}\\right\\}$ is given by a *pair* of heterophilous dimensions: $y, x$.\n",
    "\n",
    "\n",
    "Let $\\hat{y}$ be that homophilous \"gap\" Blau dimension that maximizes the argument above. For notational simplicity, let $\\rho_{\\hat{y}} = \\rho, k_{\\hat{y}} = k$. Then we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda_1 = n\\omega\\frac{k\\rho-1}{k-1}\n",
    "\\end{equation}\n",
    "\n",
    "Now from Cheeger's inequalities:\n",
    "\n",
    "\\begin{equation}\n",
    "n\\omega\\frac{1-\\rho}{1-1/k} \\le h(A) \\le n\\omega\\sqrt{2\\frac{1-\\rho}{1-1/k}}\n",
    "\\end{equation}\n",
    "\n",
    "In the absence of any homophilous dimensions, (albeit unlikely in a social scenario), let $\\hat{y},\\hat{x}$ be that pair of heterophilous \"gap\" Blau dimensions that maximizes the argument above. For notational simplicity, let $\\rho_{\\hat{y}} = \\rho_{\\hat{x}} = \\rho, k_{\\hat{y}}=k_{\\hat{x}} = k$. Then we have:\n",
    "\n",
    "\\begin{equation}\n",
    "n\\omega\\frac{1-\\rho^2-2(1-\\rho)/k}{(1-1/k)^2} \\le h(A) \\le n\\omega\\frac{\\sqrt{2(1-\\rho^2-2(1-\\rho)/k)}}{1-1/k}\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, a highly homophilous dimension, or a pair of highly heterophilous dimensions, decrease (increase) both bounds by having very high or very low proportion of friends similar to them respectively, thus increasing (decreasing) the bottleneckedness.\n",
    "\n",
    "Interestingly, if we consider (theoretically) **exactly one dimension ($m=1$), which happens to be heterophilous** then we have a very different scenario wherein the second-largest eigenvalue is negative:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda_1 = n\\omega\\frac{k\\rho-1}{k-1} < 0\n",
    "\\end{equation}\n",
    "\n",
    "And thus we have that the smaller is $\\rho$, the higher the bounds on $h(A)$. That is, having a lower proportion of friends similar to them decreases the bottleneckedness.\n",
    "\n",
    "#### Ramanujan Graphs: Excellent Expanders\n",
    "\n",
    "If the upper bound can be asymptotically increased, that is if the spectral gap is somehow as large as it possibly can be, then we can obtain an excellent expander graph (sparse yet highly connected). This is referred to as a Ramanujan graph. For a d-regular graph, let $\\lambda_R = \\mathrm{max}_{|\\lambda_i|<d} |\\lambda_i|$. Then $\\lambda_R$ is shown to satisfy $\\lambda_R \\ge 2\\sqrt{d-1} - o(1)$. Thus, a d-regular graph $A$ is a Ramanujan Graph if $\\lambda_R \\le 2\\sqrt{d-1}$.\n",
    "\n",
    "Let $\\hat{y}$ be the most anti-ambiphilous (most likey homophilous) dimension, corresponding to $\\lambda_R$. Let $\\delta = \\frac{2\\sqrt{d-1}}{d}$. Then for our simple SBM kernel, we can derive the following conditions for being a Ramanujan Graph.\n",
    "\n",
    "If $\\hat{y}$ is an extremely homophilous dimension:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho \\le \\frac{1}{k} (1+\\delta(k-1))\n",
    "\\end{equation}\n",
    "\n",
    "If $\\hat{y}$ is an extremely heterophilous dimension:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho \\ge \\frac{1}{k} (1-\\delta(k-1))\n",
    "\\end{equation}\n",
    "\n",
    "This condition shows an \"acceptable\" limit of $\\pm\\delta(1-1/k)$ on $\\rho$ of the most extremely anti-ambiphilous dimension, so as to keep the network as an excellent expander. Clearly, $0 \\le \\rho \\le 1$, thus the first constraint is needed only when $\\delta < 1$, and the latter only when $\\delta < \\frac{1}{k-1}$. Now for $d>2$, we have $\\delta < 1$ but $\\delta > \\frac{1}{k-1} \\forall k\\ge 2$.\n",
    "\n",
    "Therefore, if the most anti-ambiphilous dimension is heterophilous, then the graph is always a Ramanujan Graph. However if it is homophilous, which is the more likely case for social networks anyway, then the graph is a Ramanujan Graph only upto a certain limit on homophily: if $\\rho \\le \\frac{1}{k} (1+\\delta(k-1))$. Moreover, in the asymptotic limit of this remaining constraint, as $d \\rightarrow \\infty, \\delta \\rightarrow 0$, and the RHS  $\\rightarrow 1/k$. Which means that for a fairly large-degree network the condition of being a Ramanujan graph approaches the condition of heterophily. **In essence, more heterophily can never be hurtful to well-connectedness.**\n",
    "\n",
    "Let's go back to the remaining constraint. Quite often, $k$ is a large number, and more importantly \"uncontrollable\" for people as social agents. Thus if we let $k\\rightarrow \\infty$, we can obtain a safe upper limit on $\\rho$ that will ensure the graph to be an excellent expander. This transforms the constraint into :\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\rho & \\le\\delta \\\\\n",
    "\\rho d & \\le 2\\sqrt{d-1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\rho$ is a representation of the proportion of friends, while $d$ of the number of friends, we obtain a very simple and \"controllable\" mechanism to ensure that the graph is an excellent expander: **have no more than twice the square root of number of friends over one be similar to you** in the most homophilous Blau dimension.\n",
    "\n",
    "#### Does USoc suggest Britain is an excellent expander?\n",
    "\n",
    "For the USoc dataset (Wave 3), we have average number of friends $d\\approx 5$. Thus on the average we'd like $\\rho \\le \\frac{4}{5} + \\frac{1}{5k}$. We notice that for all the dimensions under consideration, **age, sex, distance, education, employment, income** satisfy this threshold, but **ethnicity** doesn't. Now, we also note that ethnicity ends up contributing to the second-largest eigenvalue of $A$ (it is the most anti-ambiphilous dimension). **Therefore Britain is not an excellent expander graph, (but almost one).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond A \"d-regular\" SBM Model\n",
    "\n",
    "The models considered above, including the most generalized ones, make the assumption of every community having the same \"mean degree\" $n\\omega$, which as we saw above induces a d-regular graph where $d \\approx n\\omega$. That is, even the most generalized SBM above essentially assumes a \"re-wiring\" of a random d-regular graph, in a way that supports preferential attachment and detachment based on the categorical Blau space. While this captures most intuitions of social tie-formation --particularly homophily and core-priphery structures-- a more \"realistic\" model should capture interesting degree distributions like the power law. The need for such an assumption becomes more crucial when we consider Blau dimensions that can potentially discriminate between the number of friends people make, in particular, age.\n",
    "\n",
    "We can extend the above setup to this end. Consider $\\omega$, which up until now was a scalar safely outside all the matrix manipulations. This was capturing the mean edge-density of the society as a whole: if there are $f$ \"directed friendships\"(thus $f/2$ undirected edges) then $\\omega\\approx\\frac{f}{n^2}$ for a simple graph. But one could \"partition\" the society into \"young\" and \"old\" people, based on the \"age\" Blau dimension, and calculate separate number of friendships $f_y$ for all $n_y$ number of \"young\" people, and $f_o$ for all $n_o$ number of \"old\" people, where clearly $f_y+f_o=f, n_y+n_o=n$. Equivalently, consider $\\pi_y = n_y/n$ is the proportion of young and $\\pi_o=n_o/n$ the proportion of old ($\\pi_y+\\pi_o=1$), then we have:\n",
    "\n",
    "\\begin{equation}\n",
    "n\\omega = <f_y>\\pi_y+<f_o>\\pi_o\n",
    "\\end{equation}\n",
    "\n",
    "That is, let us note $n\\omega$ as the mean degree, and equate it to the expected degree conditioned on a particular Blau dimension (here: age). Now, nothing stops us from picking another Blau dimension, say sex, to find a new partition of the population, and evaluate corresponding conditional expected degrees. Clearly, the expected degree of the society *overall* still remains $n\\omega$, but we have now incorporated degrees within the SBM block structure.\n",
    "\n",
    "To see this more clearly, consider $\\{\\Omega_y\\}_{y=1}^m$ as a sequence of diagonal matrices, all with non-negative values such that it follows this trace-constraint:\n",
    "\\begin{equation}\n",
    "\\mathrm{tr}(\\Pi_1\\Omega_1)=\\mathrm{tr}(\\Pi_2\\Omega_2) \\dots \\mathrm{tr}(\\Pi_m\\Omega_m) = n^2\\omega = f\n",
    "\\end{equation}\n",
    "where $(\\Omega_y)_{ii}$ is the mean degree of the $i$th community in the $y$th Blau dimension. Now for the generalized SBM model, putting overall $\\Omega = \\frac{1}{(n\\omega)^{m-1}}\\bigotimes_{y=1}^m\\Omega_y$, we have the following definition:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi\\Psi &= \\frac{1}{(n\\omega)^{m-1}}\\bigotimes_{y=1}^m \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\Omega_y \\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\\\\n",
    "\\Pi\\mathcal{L} &= \\frac{1}{(n\\omega)^{m-1}}\\left\\{\\bigotimes_{y=1}^m \\mathrm{diag}\\left(\\Omega_y\\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2} \\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\boldsymbol{j}_{k_y}\\right) - \\bigotimes_{y=1}^m\\mathrm{diag}(\\boldsymbol{\\hat{\\pi}}_y) \\Omega_y\\frac{\\overrightarrow{\\Psi_y} +\\overrightarrow{\\Psi_y}^T}{2}\\right\\}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that setting $\\Omega_y=n\\omega*\\mathrm{I}$ brings us back to the original formulation.\n",
    "\n",
    "### Inference of $\\Omega$\n",
    "\n",
    "As discussed for the parameters $\\rho$, $\\Omega$ is even more straightforward to infer from social surveys that ask questions of the kind \"how many close friends do you have?\", since it is simply the mean number of friends of the community of interest. If we have $m>1$ dimensions, a question that arises is whether we should infer $\\prod_{y=1}^mk_y$ number of parameters for $\\Omega$, or simply $\\sum_{y=1}^mk_y$ parameters for $\\{\\Omega_1, \\Omega_2, \\dots \\Omega_m\\}$ wherein every dimension is treated independently. If we have a large amount of data, the former might be preferred over the latter. However, as already indicated above, treating the dimensions independently (as we also did for estimating $\\boldsymbol{\\pi}_y$) makes analysis easier thanks to nice properties of the Kroenecker product. Moreover, we notice that using the Kroenecker product is akin to the expression of $\\omega$ for a particular m-dimensional community as being the geometric mean of $\\omega$ across each of those dimensions independently. This is preferable, over say the arithmetic mean, for a few reasons:\n",
    "\n",
    "1. A $0$ value of $\\omega$ across even a single dimension should mean overall $\\omega=0$.\n",
    "2. A multiplicative scale-up in $\\omega$ in one dimension is perfectly cancelled by an identical but opposite multiplicative scale-down in $\\omega$ in another dimension.\n",
    "3. This formulation straightforwardly respects the trace-constraint overall:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Pi\\Omega &= \\frac{1}{n^{m-1}}\\bigotimes_{y=1}^m\\Pi_y * \\frac{1}{(n\\omega)^{m-1}} \\bigotimes_{y=1}^m\\Omega_y \\\\\n",
    "&= \\frac{1}{(n^2\\omega)^{m-1}}\\bigotimes_{y=1}^m\\Pi_y \\Omega_y \\\\\n",
    "\\implies\\mathrm{tr}(\\Pi\\Omega) &= \\frac{1}{(n^2\\omega)^{m-1}}\\mathrm{tr}\\left(\\bigotimes_{y=1}^m\\Pi_y \\Omega_y\\right) \\\\\n",
    "&= \\frac{1}{(n^2\\omega)^{m-1}}\\prod_{y=1}^m\\mathrm{tr}\\left(\\Pi_y \\Omega_y\\right) \\\\\n",
    "&= \\frac{1}{(n^2\\omega)^{m-1}}(n^2\\omega)^m \\quad\\quad\\text{(from above: }\\mathrm{tr}(\\Pi_1\\Omega_1)=\\mathrm{tr}(\\Pi_2\\Omega_2) \\dots \\mathrm{tr}(\\Pi_m\\Omega_m) = n^2\\omega \\text{ )}\\\\\n",
    "&= n^2\\omega\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. [Matrix perturbation theory](https://en.wikipedia.org/wiki/Eigenvalue_perturbation) & [Weyl's Inequalities](https://en.wikipedia.org/wiki/Weyl%27s_inequality) applied to see difference between:\n",
    "    1. $\\mathrm{eig}(\\mathbb{E}[A])$ and $\\mathrm{eig}(\\mathbb{E}[\\hat{A}])$\n",
    "    2. $\\mathrm{eig}(\\mathbb{E}[A])$ and $\\mathrm{eig}(A)$ (use notion of [weighted graphs](https://www.math.ucsd.edu/~fan/research/cb/ch1.pdf)?)\n",
    "2. Relationship for [Expectation of Diameter for regular graph](https://en.wikipedia.org/wiki/Regular_graph) and $\\rho, k$\n",
    "\n",
    "## See Also:\n",
    "\n",
    "0. [Spectra of general random graphs](http://www.math.cmu.edu/~mradclif/papers/spectrarandomgraphs.pdf)\n",
    "1. [Spectra of random matrices](https://web.mit.edu/18.338/www/2012s/projects/yz_slides.pdf)\n",
    "2. [Laplacian spectrum of graphs](https://www.fmf.uni-lj.si/~mohar/Papers/Spec.pdf)\n",
    "3. [Largest eigenvalues of Laplacian](https://math.mit.edu/~biriarte/Spectral_Laplacian.pdf)\n",
    "4. [Spectra of random graphs with arbitrary degrees](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.87.012803)\n",
    "5. [Kroenecker graphs](https://arxiv.org/pdf/0812.4905.pdf)\n",
    "6. [Expander graphs](http://www.cs.huji.ac.il/~nati/PAPERS/expander_survey.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eig_bounds(n=1000, e=0.5, d=5):\n",
    "    degree_lb = (4/9)*np.log(2*n/e)\n",
    "    assert (d>=degree_lb)\n",
    "    assert (0<=e<=1)\n",
    "    adj_eigerr = np.sqrt(4*d*np.log(2*n/e))\n",
    "    return (degree_lb, adj_eigerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.68624428448979, 12.879479523724573)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_bounds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
